{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# elec0135-assignment-cld\n\n## Workings Notebook\n\nI found it useful to work in Kaggle (given the 30 hours free per week of GPU time), then separate out the code into modules.\n\nI've kept this notebook in the repo to show a track record of commits and for my own future reference.\n\n### Kaggle Specific Code","metadata":{}},{"cell_type":"code","source":"# # Useful cleanups to reset status\n# !rm -rf /kaggle/working/data\n# !rm /kaggle/working/data.zip\n# !rm results.csv\n# !rm -rf /kaggle/working/artefacts","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:18:22.499807Z","iopub.execute_input":"2024-04-05T16:18:22.500284Z","iopub.status.idle":"2024-04-05T16:18:22.507366Z","shell.execute_reply.started":"2024-04-05T16:18:22.500252Z","shell.execute_reply":"2024-04-05T16:18:22.505340Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# also required in the `/interactive_runner.ipynb`\n#!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:18:22.514508Z","iopub.execute_input":"2024-04-05T16:18:22.515473Z","iopub.status.idle":"2024-04-05T16:18:39.201998Z","shell.execute_reply.started":"2024-04-05T16:18:22.515415Z","shell.execute_reply":"2024-04-05T16:18:39.199602Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: gdown in /opt/conda/lib/python3.10/site-packages (5.1.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.2.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### `/model/util.py`","metadata":{}},{"cell_type":"code","source":"\"\"\"\nFunctions for creating and training models, used across the various tasks.\n\"\"\"\nimport keras\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom tensorflow.data import Dataset\nfrom tensorflow.keras import layers, callbacks\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import BatchNormalization, Conv2D, Dense, Dropout, Flatten, GlobalAveragePooling2D, MaxPooling2D\nfrom typing import NamedTuple, Tuple\n\n\nclass Params(NamedTuple):\n    \"\"\"\n    Job Parameters Struct\n    \"\"\"\n    image_size: int\n    batch_size: int\n    epochs: int\n    epsilon: float\n    early_stopping: bool\n    early_stopping_patience: int\n    adjust_learning_rate: bool\n    opt: type\n        \n        \nclass ResultCollector():\n    \"\"\"\n    Utility class to collect up and output results from tasks.\n    \"\"\"\n    \n    TRAIN_DETAILS_FILE = \"train_details.csv\"\n    TEST_SCORES_FILE = \"test_scores.csv\"\n    \n    def __init__(\n        self,\n        path: Path\n    ):\n        self.path = path\n        self.train_details = pd.DataFrame\n        self.test_scores = pd.DataFrame\n        \n    def get_path(self) -> Path:\n        return self.path\n\n    def add_task_results(self, df_train, df_test) -> None:\n        self.add_train_details(df_train)\n        self.add_test_scores(df_test)\n        \n    def add_train_details(self, df: pd.DataFrame) -> None:\n        if self.train_details.empty:\n            self.train_details = df\n        else:\n            self.train_details = pd.concat([self.train_details, df])\n        \n        self._save(self.train_details, self.TRAIN_DETAILS_FILE)        \n\n    def get_train_details(self) -> pd.DataFrame:\n        return self.train_details\n    \n    def add_test_scores(self, df: pd.DataFrame) -> None:\n        if self.test_scores.empty:\n            self.test_scores = df\n        else:\n            self.test_scores = pd.concat([self.test_scores, df])\n            \n        self._save(self.test_scores, self.TEST_SCORES_FILE)\n            \n    def get_test_scores(self) -> pd.DataFrame:\n        return self.test_scores\n    \n    def restore_results(self, quietly = True) -> None:\n        try:\n            self.train_details = pd.read_csv(self.path / self.TRAIN_DETAILS_FILE)\n            self.test_scores = pd.read_csv(self.path / self.TEST_SCORES_FILE)\n        except FileNotFoundError:\n            print(\"Unable to restore history - starting fresh\")\n            if not quietly:\n                raise\n    \n    def _save(self, df: pd.DataFrame, name: str) -> None:\n        df.to_csv(self.path / name, index=False)\n\n\n@dataclass\nclass ModelWrapper():\n    \"\"\"\n    Utility class to hold the \"outer\" model, and the inner base model\n    so that training can be fine-tuned if required.\n    \"\"\"    \n    model: keras.Model\n    base_model: keras.Model\n\n\ndef create_model(base_model_fn: str, params: Params, fc_layers = 2, fc_neurons = 1024, bn = False) -> ModelWrapper:\n    \"\"\"\n    Create Keras application model, e.g.\n        tf.keras.applications.EfficientNetV2B0\n        tf.keras.applications.ConvNeXtBase\n    with a custom top.\n    \"\"\"\n    inputs = keras.Input(shape=(params.image_size, params.image_size, 3))\n    # Base\n    base_model = base_model_fn(weights='imagenet', include_top=False)\n    base_model.trainable = False\n    # set training=F here per https://keras.io/guides/transfer_learning/\n    x = base_model(inputs, training=False)\n    # Head\n    x = GlobalAveragePooling2D()(x)\n    if bn:\n        x = BatchNormalization()(x)\n    x = Flatten()(x)\n    \n    l = 0\n    while (l < fc_layers):\n        x = Dense(fc_neurons, activation=\"relu\")(x)\n        x = Dropout(0.5)(x)\n        l = l + 1\n    \n    outputs = Dense(5, activation=\"softmax\")(x)\n    model = keras.Model(inputs, outputs)\n\n    return ModelWrapper(model, base_model)\n\n\ndef run_task(task_id: str, model_wrapper: ModelWrapper,\n             ds_train: Dataset, ds_valid: Dataset, ds_test: Dataset,\n             params: Params, collector: ResultCollector, weights = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \n    model = model_wrapper.model\n    # train\n    start = datetime.datetime.now()\n    df_train = train(task_id, model, ds_train, ds_valid, params)\n    end = datetime.datetime.now()\n    # test\n    test_result = model.evaluate(ds_test)\n    df_test = create_test_record(task_id, test_result, (end-start))\n    # save CM too\n    save_confusion_matrix(collector.get_path(), ds_test, model, task_id)\n    return df_train, df_test\n\n\ndef train(task_id: str, model: Model,\n             ds_train_: Dataset, ds_valid_: Dataset,\n             params: Params, weights = None) -> pd.DataFrame:\n    \n    opt = params.opt\n    print(f\"Using: {opt}\")\n    model.compile(\n        optimizer=params.opt(epsilon=params.epsilon),\n        loss=\"categorical_crossentropy\",\n        metrics=['accuracy']\n    )\n\n    early_stopping = callbacks.EarlyStopping(\n        min_delta=0.0001,\n        patience=params.early_stopping_patience,\n        restore_best_weights=True,\n        verbose = 1\n    )\n    \n    reduce_lr = callbacks.ReduceLROnPlateau(\n        monitor = 'val_loss', factor = 0.3, \n        patience = 3, min_delta = 0.0005, \n        mode = 'min', verbose = 1)\n    \n    cbs = []\n    if params.early_stopping:\n        print(\"Using EarlyStopping\")\n        cbs += [early_stopping]\n    if params.adjust_learning_rate:\n        print(\"Using ReduceLROnPlateau\")\n        cbs += [reduce_lr]\n\n    assert 1==2, \"break\"\n\n    history = model.fit(\n        ds_train_,\n        validation_data=ds_valid_,\n        epochs=params.epochs,\n        verbose=1,\n        callbacks=cbs,\n        class_weight=weights\n    )\n   \n    df_hist = pd.DataFrame(history.history)\n    df_hist[\"task_id\"] = task_id\n    df_hist[\"epoch\"] = df_hist.index\n   \n    return df_hist\n\n\ndef create_test_record(task_id: str, result: list[float], duration: timedelta):\n    return pd.DataFrame({\"task_id\": [task_id], \"test_loss\" : [result[0]], \"time_secs\": [duration.seconds]})\n\n\ndef save_confusion_matrix(path: Path, ds: Dataset, model: Model, task_id: str) -> None:\n    filepath = f\"artefacts/conf_mat_{task_id}.png\"\n    filepath = path / filepath\n    \n    probabilities = model.predict(ds)\n    predictions = np.argmax(probabilities, axis=1)\n\n    one_hot_labels = np.concatenate([y for x, y in ds], axis=0)\n    labels = [np.argmax(x) for x in one_hot_labels]\n    \n    result = confusion_matrix(labels, predictions, labels=[0,1,2,3,4], normalize='pred')\n    disp = ConfusionMatrixDisplay(result, display_labels=[0,1,2,3,4])\n    disp.plot()\n    disp.ax_.set_title(task_id)\n    \n    print(f\"Saving confusion matrix to {path}\")\n    disp.figure_.savefig(filepath, dpi=300)\n    \n    \ndef create_vgg_like_model(params: Params) -> ModelWrapper:\n    inputs = keras.Input(shape=(params.image_size, params.image_size, 3))\n    x = Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n    x = MaxPooling2D(pool_size=(2,2))(x)\n    x = Dropout(0.25)(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = MaxPooling2D(pool_size=(2,2))(x)\n    x = Dropout(0.25)(x)\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = MaxPooling2D(pool_size=(2,2))(x)\n    x = Dropout(0.25)(x)\n\n    # classification layers\n    x = Flatten()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.5)(x)\n\n    outputs = Dense(5, activation=\"softmax\")(x)\n    model = keras.Model(inputs, outputs)\n\n    return ModelWrapper(model, None)\n\n\n\ndef create_simple_model(params: Params) -> Model:\n    m = keras.Sequential([\n        \n        tf.keras.Input(shape=(params.image_size, params.image_size, 3)),\n        \n        # First Convolutional Block\n        layers.Conv2D(filters=32, kernel_size=5, activation=\"relu\", padding='same'),\n        layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\", padding='same'),\n        layers.MaxPool2D(),\n        layers.Dropout(0.2),\n\n        # Second Convolutional Block\n        layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\", padding='same'),\n        layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\", padding='same'),\n        layers.MaxPool2D(),\n        layers.Dropout(0.2),\n\n        # Third Convolutional Block\n        layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding='same'),\n        layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding='same'),\n        layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding='same'),\n        layers.MaxPool2D(),\n        layers.Dropout(0.2),\n\n        # Classifier Head\n        layers.Flatten(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(units=5, activation=\"softmax\"),\n    ])\n    return ModelWrapper(m, None)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:21:37.073533Z","iopub.execute_input":"2024-04-05T16:21:37.074073Z","iopub.status.idle":"2024-04-05T16:21:37.142806Z","shell.execute_reply.started":"2024-04-05T16:21:37.074034Z","shell.execute_reply":"2024-04-05T16:21:37.140705Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"### `/data/data_processing.py`","metadata":{}},{"cell_type":"code","source":"import gdown\nimport keras\nimport pandas as pd\nimport random\nimport shutil\nimport tensorflow as tf\nimport os\nimport zipfile\n\n# handle different structure Kaggle (Notebook) vs. Colab (Modules)\n# this wouldn't be kept in any \"production\" version.\ntry:\n    from AMLS_II_assignment23_24.model.util import Params\nexcept ModuleNotFoundError:\n    pass\n\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.data import Dataset\nfrom tensorflow.data.experimental import AUTOTUNE\nfrom tensorflow.keras import layers, callbacks\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom typing import Tuple\n\n\ndef data_preprocessing(path: Path,\n                       params: Params,\n                       force=False) -> Tuple[Dataset, Dataset, Dataset, dict]:\n    \"\"\"\n    \"\"\"\n    file = download_data(path, force)\n    \n    data_path = path / \"data\"\n    if force:\n        shutil.rmtree(data_path)\n        \n    if not data_path.exists():\n        data_path.mkdir(parents=True, exist_ok=True)\n       \n        with zipfile.ZipFile(file, \"r\") as z:\n            z.extractall(data_path)\n        \n    df_images = pd.read_csv((data_path / \"train.csv\"))\n    \n    imgs1 = random.sample(df_images[df_images.label==3].image_id.tolist(), k=2577)\n    imgs2 = df_images[df_images.label!=3].image_id.tolist()\n    \n    df_images = df_images[df_images.image_id.isin((imgs1+imgs2))].copy()\n    \n    X_train, X_test, y_train, y_test = train_test_split(df_images.image_id, df_images.label, test_size=0.2, random_state=12)\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25, random_state=12)\n    \n    train_path = create_ds_tree(X_train, y_train, data_path, \"train\")\n    valid_path = create_ds_tree(X_valid, y_valid, data_path, \"valid\")\n    test_path = create_ds_tree(X_test, y_test, data_path, \"test\")\n    \n    ds_train = create_dataset(train_path, params.image_size, params.batch_size)\n    ds_valid = create_dataset(valid_path, params.image_size, params.batch_size)\n    ds_test = create_dataset(test_path, params.image_size, params.batch_size, False)\n\n    return ds_train, ds_valid, ds_test, extract_class_weights(df_images)\n\n\ndef download_data(path: Path, force=False) -> Path:\n    \"\"\"\n    \"\"\"\n    url = \"https://drive.google.com/uc?id=1TJBf1HZxAMpowZ92BcgS5N_NPHE7LPOT\"\n    output = path / \"data.zip\"\n    if not Path(output).exists() or force:\n        gdown.download(url, str(output), quiet=False)\n    return output\n\n\ndef create_ds_tree(x, y, path: Path, name: str) -> Path:\n    \"\"\"\n    Creates the directory structure for the given dataset.\n    \"\"\"\n    ds_path = path / name\n    if not ds_path.exists():\n        ds_path.mkdir(parents=True, exist_ok=True)\n\n        for lab in y.unique():\n            (ds_path / str(lab)).mkdir(exist_ok=True)\n\n        source_path = path / \"train_images\"\n        \n        for img, lab in zip(x, y):\n            src = source_path / img\n            dest = ds_path / str(lab) / img\n            shutil.move(src, dest)\n        \n    return ds_path\n\n\ndef create_dataset(path: Path, img_size: int, batch_size: int, shuffle = True) -> Dataset:\n    \"\"\"\n    \"\"\"\n    return image_dataset_from_directory(\n        path,\n        labels='inferred',\n        label_mode='categorical',\n        image_size=[img_size, img_size],\n        batch_size=batch_size,\n        seed=12345,\n        shuffle=shuffle,\n        crop_to_aspect_ratio=True\n    )\n\n\ndef extract_class_weights(df_data: pd.DataFrame) -> dict:\n    classes = df_data.label.unique()\n    class_weights = compute_class_weight(class_weight='balanced',\n                                         classes=classes,\n                                         y=df_data.label)\n\n    return dict(zip(classes, class_weights))\n\n\ndef convert_dataset(ds: Dataset) -> Dataset:\n    \"\"\"\n    \"\"\"\n    def convert_to_float(image, label):\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        image = image / 255.0\n        return image, label\n\n    return (\n        ds\n        .map(convert_to_float)\n        .cache()\n        .prefetch(buffer_size=AUTOTUNE)\n    )\n    \n\ndef augment_dataset_old(ds: Dataset, num_repeats: int) -> Dataset:\n    \"\"\"\n    \"\"\"\n    def augment(image, label):\n        seed = 12345\n        image = tf.image.random_flip_left_right(image, seed)\n        image = tf.image.random_flip_up_down(image, seed)\n        image = tf.image.random_brightness(image, 0.2, seed)\n        return image, label\n\n    return (\n        ds\n        .repeat(num_repeats)\n        .map(augment)\n        .cache()\n        .prefetch(buffer_size=AUTOTUNE)\n    )\n\n\ndef augment_dataset(ds: Dataset, num_repeats: int) -> Dataset:\n    \"\"\"\n    \"\"\"\n    def augment(image, label):\n        seed = 12345\n        image = tf.image.random_flip_left_right(image, seed)\n        image = tf.image.random_flip_up_down(image, seed)\n        image = tf.image.random_brightness(image, 0.2, seed)\n        return image, label\n\n    return (\n        ds\n        .repeat(num_repeats)\n        .map(augment)\n    )\n\ndef over_sample_class(ds: Dataset, class_label: int, batch_size: int, num_repeats: int = 1) -> Dataset:\n    # filter dataset to just the class_label\n    ds_filt = ds.unbatch().filter(lambda x, label: tf.equal(tf.argmax(label, axis=0), class_label))\n    ds_filt = ds.repeat(num_repeats)\n    # combined with original dataset, re-shuffle, and re-batch\n    ds_over = tf.data.Dataset.concatenate(ds.unbatch(), ds_filt)\n    ds_over = ds_over.shuffle(100000)\n    ds_over = ds_over.batch(batch_size)\n    return ds_over\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:21:38.159777Z","iopub.execute_input":"2024-04-05T16:21:38.160667Z","iopub.status.idle":"2024-04-05T16:21:38.201273Z","shell.execute_reply.started":"2024-04-05T16:21:38.160615Z","shell.execute_reply":"2024-04-05T16:21:38.199592Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"### `/report.py`?","metadata":{"execution":{"iopub.status.busy":"2024-04-05T10:37:47.062863Z","iopub.execute_input":"2024-04-05T10:37:47.063224Z","iopub.status.idle":"2024-04-05T10:37:47.067978Z","shell.execute_reply.started":"2024-04-05T10:37:47.063198Z","shell.execute_reply":"2024-04-05T10:37:47.066722Z"}}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_experiments_comp2(df_history: pd.DataFrame, task_ids: list, epoch_limit = 50) -> None:\n    df = df_history[(df_history.task_id.isin(task_ids)) & (df_history.epoch <= epoch_limit)].copy()\n    df[\"loss_gap\"] = df.val_loss - df.loss\n    df_grp = df[[\"epoch\",\"task_id\", \"val_accuracy\", \"val_loss\", \"loss_gap\"]].groupby([\"epoch\", \"task_id\"]).mean()\n    fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(16, 8))\n    sns.lineplot(data=df_grp, x=\"epoch\", y=\"val_accuracy\", hue=\"task_id\",  ax=ax1)\n    sns.lineplot(data=df_grp, x=\"epoch\", y=\"val_loss\", hue=\"task_id\",  ax=ax2)\n    sns.lineplot(data=df_grp, x=\"epoch\", y=\"loss_gap\", hue=\"task_id\",  ax=ax3)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:21:39.490086Z","iopub.execute_input":"2024-04-05T16:21:39.490569Z","iopub.status.idle":"2024-04-05T16:21:39.500009Z","shell.execute_reply.started":"2024-04-05T16:21:39.490534Z","shell.execute_reply":"2024-04-05T16:21:39.498899Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### `/main.py`","metadata":{}},{"cell_type":"code","source":"x = tf.keras.optimizers.Adam","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:18:39.360960Z","iopub.execute_input":"2024-04-05T16:18:39.361571Z","iopub.status.idle":"2024-04-05T16:18:39.377235Z","shell.execute_reply.started":"2024-04-05T16:18:39.361521Z","shell.execute_reply":"2024-04-05T16:18:39.375747Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"type(x)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:18:39.379902Z","iopub.execute_input":"2024-04-05T16:18:39.380462Z","iopub.status.idle":"2024-04-05T16:18:39.394302Z","shell.execute_reply.started":"2024-04-05T16:18:39.380411Z","shell.execute_reply":"2024-04-05T16:18:39.392561Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"type"},"metadata":{}}]},{"cell_type":"code","source":"import datetime\nimport os\nimport pandas as pd\nimport tensorflow as tf\n\n# handle different structure Kaggle (Notebook) vs. Colab (Modules)\n# this wouldn't be kept in any \"production\" version.\ntry:\n    from AMLS_II_assignment23_24.data_processing.pre_processing import data_preprocessing, rebatch\n    from AMLS_II_assignment23_24.model import util as model_util\n    from AMLS_II_assignment23_24.model.util import Params, ResultCollector, create_model\nexcept ModuleNotFoundError:\n    pass\n\nfrom docopt import docopt\nfrom pathlib import Path\nfrom tensorflow.keras.optimizers import Adam, AdamW\n\ntf.random.set_seed(67890)\n\n# Starting set of params\nparams = Params(255, 196, 50, 0.005, True, 7, False, Adam)\n\nARTEFACTS_PATH = Path(\"artefacts\")\nARTEFACTS_PATH.mkdir(parents=True, exist_ok=True)\n\ncollector = ResultCollector(ARTEFACTS_PATH)\ncollector.restore_results()\n\n# Process Data\nprint(\"================\")\nprint(\"= Loading Data =\")\nprint(\"================\")\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(os.getcwd()), params)\nprint(f\"Class Weights: {class_weights}\\n\\n\")\n\nprint(\"==== Task A: Explore Batch Size ====\")\nfor bs in [64, 128, 192, 256]:\n    print(f\"Batch Size: {bs}\")\n    ds_train = ds_train.rebatch(bs)\n    ds_valid = ds_valid.rebatch(bs)\n    model = create_model(tf.keras.applications.ConvNeXtTiny, params) # can use default params here\n    run_task(f\"A_{bs}\", model, ds_train, ds_valid, ds_test, params, collector)\n\n# update based on results of Task A\nparams = Params(255, 256, 50, 0.005, True, 7, False, Adam)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\n\nprint(\"==== Task B: Explore Epsilon ====\")\nfor e in [0.0025, 0.0050, 0.0075, 0.01]:\n    print(f\"Epsilon: {e}\")\n    p = Params(255, 256, 50, e, True, 7, False, Adam)    \n    model = create_model(tf.keras.applications.ConvNeXtTiny, p)\n    #run_task(f\"B_{e}\", model, ds_train, ds_valid, ds_test, collector, p)\n\n# update based on results of Task B\nparams = Params(255, 256, 50, 0.0075, True, 7, False, Adam)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\n\nprint(\"==== Task C: Baseline Model Comparison ====\")\nfor m in [tf.keras.applications.ConvNeXtTiny, tf.keras.applications.ConvNeXtBase,\n          tf.keras.applications.EfficientNetB0, tf.keras.applications.EfficientNetV2B0]:\n    print(f\"Model: {m}\")\n    model = create_model(m, params)\n    # run_task(f\"C_{model.base_model.name}\", model, ds_train, ds_valid, ds_test, collector, params)\n\nprint(\"==== Task D: Best-of-Breed Model ====\")\nparams = Params(255, 256, 50, 0.0075, True, 7, False, AdamW)\n# oversample & augment dataset\nds_train_aug = augment_dataset(over_sample_class(ds_train, 0, params.batch_size), 2)\n# initial training\nmodel_d = create_model(tf.keras.applications.EfficientNetV2B0, params, bn=True)\nrun_task(f\"D_base\", model_d, ds_train, ds_valid, ds_test, collector, params, class_weights)\n# fine-tune by allowing base model to be re-trained\nmodel_d.base_model.trainable = True\nft_params = Params(255, 256, 50, 1e-5, True, 5, False)\nrun_task(f\"D_tuned\", model, ds_train_aug, ds_valid, ds_test, collector, ft_params, class_weights)\ndel model_d\n\nprint(\"========================\")\nprint(\"==== Ablation Study ====\")\nprint(\"========================\")\n\nprint(\"==== Task E: Remove Fine-Tuning ====\")\nmodel = create_model(tf.keras.applications.ConvNeXtBase, params, bn=True)\nrun_task(f\"E\", model, ds_train_aug, ds_valid, ds_test, collector, params, class_weights)\ndel model\n\nprint(\"==== Task F: Remove Class Weights ====\")\nmodel = create_model(tf.keras.applications.ConvNeXtBase, params, bn=True)\nrun_task(f\"F\", model, ds_train_aug, ds_valid, ds_test, collector, params)\ndel model\n\nprint(\"==== Task G: Remove Data Augmentation ====\")\nmodel = create_model(tf.keras.applications.ConvNeXtBase, params, bn=True)\nrun_task(f\"G\", model, ds_train, ds_valid, ds_test, collector, params, class_weights)\ndel model\n\nprint(\"==== Task H: Remove Batch Norm ====\")\nmodel = create_model(tf.keras.applications.ConvNeXtBase, params)\nrun_task(f\"H\", model, ds_train, ds_valid, ds_test, collector, params, class_weights)\ndel model\n\nprint(\"==== Task I: Regress to the Adam Optimiser ====\")\nparams = Params(255, 256, 50, 0.0075, True, 7, False, Adam)\nmodel = create_model(tf.keras.applications.ConvNeXtBase, params)\nrun_task(f\"I\", model, ds_train, ds_valid, ds_test, collector, params, class_weights)\ndel model\n\nprint(\"==== Task J: Remove a FC Layer ====\")\nparams = Params(255, 256, 50, 0.0075, True, 7, False, Adam)\nmodel = create_model(tf.keras.applications.ConvNeXtBase, params, 1)\nrun_task(f\"J_1\", model, ds_train, ds_valid, ds_test, collector, params, class_weights)\ndel model\n\nprint(\"==== Task J: Add a FC Layer ====\")\nparams = Params(255, 256, 50, 0.0075, True, 7, False, Adam)\nmodel = create_model(tf.keras.applications.ConvNeXtBase, params, 3)\nrun_task(f\"J_3\", model, ds_train, ds_valid, ds_test, collector, params, class_weights)\ndel model\n    \n#     df_train, df_test = run_task(f\"A_{bs}\", model, ds_train, ds_valid, ds_test, batch_size_params)\n#     collector.add_task_results(df_train, df_test)\n#     print(model.model.evaluate(ds_test))\n\n# print(\"==== Task A: Baseline Model ====\")\n\n# model = create_convnext_base(DEFAULT_PARAMS)\n# df_train, df_test = run_task(\"A_base\", model, ds_train, ds_valid, ds_test, DEFAULT_PARAMS)\n# collector.add_task_results(df_train, df_test)\n\n# print(\"==== Task B: Baseline + Data Augmentation ====\")\n# {\n#     \"\"\"\n#     Per task A, but with data augmentation.\n#     \"\"\"\n#     ds_train_aug = augment_dataset(ds_train, 2)\n#     model = create_convnext_base(DEFAULT_PARAMS)\n#     df_train, df_test = run_task(\"B_base_aug\", model, ds_train_aug, ds_valid, ds_test, DEFAULT_PARAMS)\n#     collector.add_task_results(df_train, df_test)    \n# }\n\n# print(\"==== Task C: Baseline + Data Augmentation + Class Weights ====\")\n# model = create_convnext_base(DEFAULT_PARAMS)\n# {\n#     \"\"\"\n#     Per task B but, given the large class imbalance, class weight supplied.\n#     \"\"\"\n#     ds_train_aug = augment_dataset(ds_train, 2)\n#     df_train, df_test = run_task(\"C_base_aug_wgts\", model, ds_train_aug, ds_valid, ds_test, DEFAULT_PARAMS, class_weights)\n#     collector.add_task_results(df_train, df_test)    \n# }\n\n# print(\"==== Task D: Baseline + Data Augmentation + Class Weights + Fine Tune ====\")\n# {\n#     \"\"\"\n#     Per task C but, given the large class imbalance, class weight supplied.\n#     \"\"\"\n#     fine_tune_params = Params(50, 196, 1, 1e-5, True, 5, False)\n#     print(fine_tune_params)\n#     model.base_model.trainable = True\n#     ds_train_aug = augment_dataset(ds_train, 2)\n#     df_train, df_test = run_task(\"D_base_aug_wgts_ft\", model, ds_train_aug, ds_valid, ds_test, DEFAULT_PARAMS, class_weights)\n#     collector.add_task_results(df_train, df_test)    \n# }\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:24:16.461070Z","iopub.execute_input":"2024-04-05T16:24:16.461626Z","iopub.status.idle":"2024-04-05T16:24:19.988067Z","shell.execute_reply.started":"2024-04-05T16:24:16.461589Z","shell.execute_reply":"2024-04-05T16:24:19.986337Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Params(image_size=255, batch_size=196, epochs=50, epsilon=0.005, early_stopping=True, early_stopping_patience=7, adjust_learning_rate=False, opt=<class 'keras.src.optimizers.adam.Adam'>)\n================\n= Loading Data =\n================\nFound 6489 files belonging to 5 classes.\nFound 2163 files belonging to 5 classes.\nFound 2164 files belonging to 5 classes.\nClass Weights: {0: 1.990064397424103, 1: 0.9882137962539973, 2: 0.9066219614417435, 4: 0.8394256887854094, 3: 0.8394256887854094}\n\n\n==== Task A: Explore Batch Size ====\nBatch Size: 64\nUsing: <class 'keras.src.optimizers.adam.Adam'>\nUsing EarlyStopping\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[48], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m     ds_valid \u001b[38;5;241m=\u001b[39m ds_valid\u001b[38;5;241m.\u001b[39mrebatch(bs)\n\u001b[1;32m     43\u001b[0m     model \u001b[38;5;241m=\u001b[39m create_model(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mapplications\u001b[38;5;241m.\u001b[39mConvNeXtTiny, params) \u001b[38;5;66;03m# can use default params here\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mrun_task\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# update based on results of Task A\u001b[39;00m\n\u001b[1;32m     47\u001b[0m params \u001b[38;5;241m=\u001b[39m Params(\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m0.005\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, Adam)\n","Cell \u001b[0;32mIn[40], line 140\u001b[0m, in \u001b[0;36mrun_task\u001b[0;34m(task_id, model_wrapper, ds_train, ds_valid, ds_test, params, collector, weights)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m    139\u001b[0m start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m--> 140\u001b[0m df_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m end \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# test\u001b[39;00m\n","Cell \u001b[0;32mIn[40], line 182\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(task_id, model, ds_train_, ds_valid_, params, weights)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing ReduceLROnPlateau\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    180\u001b[0m     cbs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [reduce_lr]\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbreak\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    184\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    185\u001b[0m     ds_train_,\n\u001b[1;32m    186\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mds_valid_,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39mweights\n\u001b[1;32m    191\u001b[0m )\n\u001b[1;32m    193\u001b[0m df_hist \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(history\u001b[38;5;241m.\u001b[39mhistory)\n","\u001b[0;31mAssertionError\u001b[0m: break"],"ename":"AssertionError","evalue":"break","output_type":"error"}]},{"cell_type":"code","source":"m1 = create_model_ablations(tf.keras.applications.ConvNeXtTiny, \"base\", params, 2, 1024)\nm1.model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}