{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# elec0135-assignment-cld\n\n## Workings Notebook\n\nI found it useful to work in Kaggle (given the 30 hours free per week of GPU time), then separate out the code into modules.\n\nI've kept this notebook in the repo to show a track record of commits and for my own future reference.\n\n### Kaggle Specific Code","metadata":{}},{"cell_type":"code","source":"# # Useful cleanups to reset status\n# !rm -rf /kaggle/working/data\n# !rm /kaggle/working/data.zip\n# !rm results.csv\n# !rm -rf /kaggle/working/artefacts","metadata":{"execution":{"iopub.status.busy":"2024-04-05T10:52:15.964674Z","iopub.execute_input":"2024-04-05T10:52:15.965913Z","iopub.status.idle":"2024-04-05T10:52:15.971307Z","shell.execute_reply.started":"2024-04-05T10:52:15.965859Z","shell.execute_reply":"2024-04-05T10:52:15.969863Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# also required in the `/interactive_runner.ipynb`\n!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2024-04-05T10:52:15.973236Z","iopub.execute_input":"2024-04-05T10:52:15.973850Z","iopub.status.idle":"2024-04-05T10:52:31.778731Z","shell.execute_reply.started":"2024-04-05T10:52:15.973816Z","shell.execute_reply":"2024-04-05T10:52:31.777064Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /opt/conda/lib/python3.10/site-packages (5.1.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.2.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### `/model/util.py`","metadata":{}},{"cell_type":"code","source":"\"\"\"\nFunctions for creating and training models, used across the various tasks.\n\"\"\"\nimport keras\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom tensorflow.data import Dataset\nfrom tensorflow.keras import layers, callbacks\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import BatchNormalization, Conv2D, Dense, Dropout, Flatten, GlobalAveragePooling2D, MaxPooling2D\nfrom typing import NamedTuple, Tuple\n\n\nclass Params(NamedTuple):\n    \"\"\"\n    Job Parameters Struct\n    \"\"\"\n    image_size: int\n    batch_size: int\n    epochs: int\n    epsilon: float\n    early_stopping: bool\n    early_stopping_patience: int\n    adjust_learning_rate: bool\n        \n        \nclass ResultCollector():\n    \"\"\"\n    Utility class to collect up and output results from tasks.\n    \"\"\"\n    \n    TRAIN_DETAILS_FILE = \"train_details.csv\"\n    TEST_SCORES_FILE = \"test_scores.csv\"\n    \n    def __init__(\n        self,\n        path: Path\n    ):\n        self.path = path\n        self.train_details = pd.DataFrame\n        self.test_scores = pd.DataFrame\n\n    def add_task_results(self, df_train, df_test) -> None:\n        self.add_train_details(df_train)\n        self.add_test_scores(df_test)\n        \n    def add_train_details(self, df: pd.DataFrame) -> None:\n        if self.train_details.empty:\n            self.train_details = df\n        else:\n            self.train_details = pd.concat([self.train_details, df])\n        \n        self._save(self.train_details, self.TRAIN_DETAILS_FILE)        \n\n    def get_train_details(self) -> pd.DataFrame:\n        return self.train_details\n    \n    def add_test_scores(self, df: pd.DataFrame) -> None:\n        if self.test_scores.empty:\n            self.test_scores = df\n        else:\n            self.test_scores = pd.concat([self.test_scores, df])\n            \n        self._save(self.test_scores, self.TEST_SCORES_FILE)\n            \n    def get_test_scores(self) -> pd.DataFrame:\n        return self.test_scores\n    \n    def restore_results(self) -> None:\n        self.train_details = pd.read_csv(self.TRAIN_DETAILS_FILE)\n        self.test_scores = pd.read_csv(self.TEST_SCORES_FILE)\n    \n    def _save(self, df: pd.DataFrame, name: str) -> None:\n        df.to_csv(self.path / name, index=False)\n\n\n@dataclass\nclass ModelWrapper():\n    \"\"\"\n    Utility class to hold the \"outer\" model, and the inner base model\n    so that training can be fine-tuned if required.\n    \"\"\"    \n    model: keras.Model\n    base_model: keras.Model\n\n        \ndef create_convnext_base(params: Params) -> ModelWrapper:\n    return create_model(tf.keras.applications.ConvNeXtBase, \"baseline\", DEFAULT_PARAMS)\n\n\ndef create_model(base_model_fn: str, name: str, params: Params) -> ModelWrapper:\n    \"\"\"\n    Create Keras application model, e.g.\n        tf.keras.applications.EfficientNetV2B0\n        tf.keras.applications.ConvNeXtBase\n    with a custom top.\n    \"\"\"\n    # i = 0\n    # name=f\"{name}-{(i:=i+1)}\"\n    # name = [name+str(i) for i in range(17)]\n    inputs = keras.Input(shape=(params.image_size, params.image_size, 3))\n    # Base\n    base_model = base_model_fn(weights='imagenet', include_top=False)\n    base_model.trainable = False\n    # set training=F here per https://keras.io/guides/transfer_learning/\n    x = base_model(inputs, training=False)\n    # Head\n    x = GlobalAveragePooling2D()(x)\n    x = Flatten()(x)\n    x = Dense(1024, activation=\"gelu\")(x)\n    x = Dropout(0.5)(x)\n    x = Dense(1024, activation=\"gelu\")(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(5, activation=\"softmax\")(x)\n    model = keras.Model(inputs, outputs)\n\n    return ModelWrapper(model, base_model)\n\n\ndef create_model_ablations(base_model_fn: str, name: str, params: Params, fc_layers = 2, fc_neurons = 1024, bn = False) -> ModelWrapper:\n    \"\"\"\n    Create Keras application model, e.g.\n        tf.keras.applications.EfficientNetV2B0\n        tf.keras.applications.ConvNeXtBase\n    with a custom top.\n    \"\"\"\n    # i = 0\n    # name=f\"{name}-{(i:=i+1)}\"\n    # name = [name+str(i) for i in range(17)]\n    inputs = keras.Input(shape=(params.image_size, params.image_size, 3))\n    # Base\n    base_model = base_model_fn(weights='imagenet', include_top=False)\n    base_model.trainable = False\n    # set training=F here per https://keras.io/guides/transfer_learning/\n    x = base_model(inputs, training=False)\n    # Head\n    x = GlobalAveragePooling2D()(x)\n    if bn:\n        x = BatchNormalization()(x)\n    x = Flatten()(x)\n    \n    l = 0\n    while (l < fc_layers):\n        x = Dense(fc_neurons, activation=\"relu\")(x)\n        x = Dropout(0.5)(x)\n        l = l + 1\n    \n    outputs = Dense(5, activation=\"softmax\")(x)\n    model = keras.Model(inputs, outputs)\n\n    return ModelWrapper(model, base_model)\n\n\ndef run_task(task_id: str, model_wrapper: ModelWrapper,\n             ds_train: Dataset, ds_valid: Dataset, ds_test: Dataset,\n             params: Params, weights = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \n    model = model_wrapper.model\n    # train\n    df_train = train(task_id, model, ds_train, ds_valid, params)\n    # test\n    test_result = model.evaluate(ds_test)\n    df_test = create_test_record(task_id, test_result)\n    # save CM too\n    save_confusion_matrix(ds_test, model, task_id)\n    return df_train, df_test\n\n\ndef train(task_id: str, model: Model,\n             ds_train_: Dataset, ds_valid_: Dataset,\n             params: Params, weights = None) -> pd.DataFrame:\n    \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(epsilon=params.epsilon),\n        loss=\"categorical_crossentropy\",\n        metrics=['accuracy']\n    )\n\n    early_stopping = callbacks.EarlyStopping(\n        min_delta=0.0001,\n        patience=params.early_stopping_patience,\n        restore_best_weights=True,\n        verbose = 1\n    )\n    \n    reduce_lr = callbacks.ReduceLROnPlateau(\n        monitor = 'val_loss', factor = 0.3, \n        patience = 3, min_delta = 0.0005, \n        mode = 'min', verbose = 1)\n    \n    cbs = []\n    if params.early_stopping:\n        print(\"Using EarlyStopping\")\n        cbs += [early_stopping]\n    if params.adjust_learning_rate:\n        print(\"Using ReduceLROnPlateau\")\n        cbs += [reduce_lr]\n\n    history = model.fit(\n        ds_train_,\n        validation_data=ds_valid_,\n        epochs=params.epochs,\n        verbose=1,\n        callbacks=cbs,\n        class_weight=weights\n    )\n   \n    df_hist = pd.DataFrame(history.history)\n    df_hist[\"task_id\"] = task_id\n    df_hist[\"epoch\"] = df_hist.index\n   \n    return df_hist\n\n\ndef create_test_record(task_id: str, result: list[float]):\n    return pd.DataFrame({\"task_id\": [task_id], \"test_loss\" : [result[0]], \"test_accuracy\": [result[1]]})\n\n\ndef save_confusion_matrix(ds: Dataset, model: Model, task_id: str) -> None:\n    path = f\"artefacts/conf_mat_{task_id}.png\"\n    probabilities = model.predict(ds)\n    predictions = np.argmax(probabilities, axis=1)\n\n    one_hot_labels = np.concatenate([y for x, y in ds], axis=0)\n    labels = [np.argmax(x) for x in one_hot_labels]\n    \n    result = confusion_matrix(labels, predictions, labels=[0,1,2,3,4], normalize='pred')\n    disp = ConfusionMatrixDisplay(result, display_labels=[0,1,2,3,4])\n    disp.plot()\n    disp.ax_.set_title(task_id)\n    \n    print(f\"Saving confusion matrix to {path}\")\n    disp.figure_.savefig(f\"artefacts/conf_mat_{task_id}.png\", dpi=300)\n    \n    \ndef create_vgg_like_model(params: Params) -> ModelWrapper:\n    inputs = keras.Input(shape=(params.image_size, params.image_size, 3))\n    x = Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n    x = MaxPooling2D(pool_size=(2,2))(x)\n    x = Dropout(0.25)(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = MaxPooling2D(pool_size=(2,2))(x)\n    x = Dropout(0.25)(x)\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = MaxPooling2D(pool_size=(2,2))(x)\n    x = Dropout(0.25)(x)\n\n    # classification layers\n    x = Flatten()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.5)(x)\n\n    outputs = Dense(5, activation=\"softmax\")(x)\n    model = keras.Model(inputs, outputs)\n\n    return ModelWrapper(model, None)\n\n\n\ndef create_simple_model(params: Params) -> Model:\n    m = keras.Sequential([\n        \n        tf.keras.Input(shape=(params.image_size, params.image_size, 3)),\n        \n        # First Convolutional Block\n        layers.Conv2D(filters=32, kernel_size=5, activation=\"relu\", padding='same'),\n        layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\", padding='same'),\n        layers.MaxPool2D(),\n        layers.Dropout(0.2),\n\n        # Second Convolutional Block\n        layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\", padding='same'),\n        layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\", padding='same'),\n        layers.MaxPool2D(),\n        layers.Dropout(0.2),\n\n        # Third Convolutional Block\n        layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding='same'),\n        layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding='same'),\n        layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding='same'),\n        layers.MaxPool2D(),\n        layers.Dropout(0.2),\n\n        # Classifier Head\n        layers.Flatten(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(units=5, activation=\"softmax\"),\n    ])\n    return ModelWrapper(m, None)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T10:52:31.782273Z","iopub.execute_input":"2024-04-05T10:52:31.782716Z","iopub.status.idle":"2024-04-05T10:52:31.845900Z","shell.execute_reply.started":"2024-04-05T10:52:31.782677Z","shell.execute_reply":"2024-04-05T10:52:31.844511Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### `/data/data_processing.py`","metadata":{}},{"cell_type":"code","source":"import gdown\nimport keras\nimport pandas as pd\nimport random\nimport shutil\nimport tensorflow as tf\nimport os\nimport zipfile\n\n# handle different structure Kaggle (Notebook) vs. Colab (Modules)\n# this wouldn't be kept in any \"production\" version.\ntry:\n    from AMLS_II_assignment23_24.model.util import Params\nexcept ModuleNotFoundError:\n    pass\n\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.data import Dataset\nfrom tensorflow.data.experimental import AUTOTUNE\nfrom tensorflow.keras import layers, callbacks\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom typing import Tuple\n\n\ndef data_preprocessing(path: Path,\n                       params: Params,\n                       force=False) -> Tuple[Dataset, Dataset, Dataset, dict]:\n    \"\"\"\n    \"\"\"\n    file = download_data(path, force)\n    \n    data_path = path / \"data\"\n    if force:\n        shutil.rmtree(data_path)\n        \n    if not data_path.exists():\n        data_path.mkdir(parents=True, exist_ok=True)\n       \n        with zipfile.ZipFile(file, \"r\") as z:\n            z.extractall(data_path)\n        \n    df_images = pd.read_csv((data_path / \"train.csv\"))\n    \n    imgs1 = random.sample(df_images[df_images.label==3].image_id.tolist(), k=2577)\n    imgs2 = df_images[df_images.label!=3].image_id.tolist()\n    \n    df_images = df_images[df_images.image_id.isin((imgs1+imgs2))].copy()\n    \n    X_train, X_test, y_train, y_test = train_test_split(df_images.image_id, df_images.label, test_size=0.2, random_state=12)\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25, random_state=12)\n    \n    train_path = create_ds_tree(X_train, y_train, data_path, \"train\")\n    valid_path = create_ds_tree(X_valid, y_valid, data_path, \"valid\")\n    test_path = create_ds_tree(X_test, y_test, data_path, \"test\")\n    \n    ds_train = create_dataset(train_path, params.image_size, params.batch_size)\n    ds_valid = create_dataset(valid_path, params.image_size, params.batch_size)\n    ds_test = create_dataset(test_path, params.image_size, params.batch_size, False)\n\n    return ds_train, ds_valid, ds_test, extract_class_weights(df_images)\n\n\ndef download_data(path: Path, force=False) -> Path:\n    \"\"\"\n    \"\"\"\n    url = \"https://drive.google.com/uc?id=1TJBf1HZxAMpowZ92BcgS5N_NPHE7LPOT\"\n    output = path / \"data.zip\"\n    if not Path(output).exists() or force:\n        gdown.download(url, str(output), quiet=False)\n    return output\n\n\ndef create_ds_tree(x, y, path: Path, name: str) -> Path:\n    \"\"\"\n    Creates the directory structure for the given dataset.\n    \"\"\"\n    ds_path = path / name\n    if not ds_path.exists():\n        ds_path.mkdir(parents=True, exist_ok=True)\n\n        for lab in y.unique():\n            (ds_path / str(lab)).mkdir(exist_ok=True)\n\n        source_path = path / \"train_images\"\n        \n        for img, lab in zip(x, y):\n            src = source_path / img\n            dest = ds_path / str(lab) / img\n            shutil.move(src, dest)\n        \n    return ds_path\n\n\ndef create_dataset(path: Path, img_size: int, batch_size: int, shuffle = True) -> Dataset:\n    \"\"\"\n    \"\"\"\n    return image_dataset_from_directory(\n        path,\n        labels='inferred',\n        label_mode='categorical',\n        image_size=[img_size, img_size],\n        batch_size=batch_size,\n        seed=12345,\n        shuffle=shuffle,\n        crop_to_aspect_ratio=True\n    )\n\n\ndef extract_class_weights(df_data: pd.DataFrame) -> dict:\n    classes = df_data.label.unique()\n    class_weights = compute_class_weight(class_weight='balanced',\n                                         classes=classes,\n                                         y=df_data.label)\n\n    return dict(zip(classes, class_weights))\n\n\ndef convert_dataset(ds: Dataset) -> Dataset:\n    \"\"\"\n    \"\"\"\n    def convert_to_float(image, label):\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n        image = image / 255.0\n        return image, label\n\n    return (\n        ds\n        .map(convert_to_float)\n        .cache()\n        .prefetch(buffer_size=AUTOTUNE)\n    )\n    \n\ndef augment_dataset(ds: Dataset, num_repeats: int) -> Dataset:\n    \"\"\"\n    \"\"\"\n    def augment(image, label):\n        seed = 12345\n        image = tf.image.random_flip_left_right(image, seed)\n        image = tf.image.random_flip_up_down(image, seed)\n        image = tf.image.random_brightness(image, 0.2, seed)\n        return image, label\n\n    return (\n        ds\n        .repeat(num_repeats)\n        .map(augment)\n        .cache()\n        .prefetch(buffer_size=AUTOTUNE)\n    )","metadata":{"execution":{"iopub.status.busy":"2024-04-05T10:52:31.847461Z","iopub.execute_input":"2024-04-05T10:52:31.848432Z","iopub.status.idle":"2024-04-05T10:52:32.154661Z","shell.execute_reply.started":"2024-04-05T10:52:31.848395Z","shell.execute_reply":"2024-04-05T10:52:32.153418Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### `/report.py`?","metadata":{"execution":{"iopub.status.busy":"2024-04-05T10:37:47.062863Z","iopub.execute_input":"2024-04-05T10:37:47.063224Z","iopub.status.idle":"2024-04-05T10:37:47.067978Z","shell.execute_reply.started":"2024-04-05T10:37:47.063198Z","shell.execute_reply":"2024-04-05T10:37:47.066722Z"}}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_experiments_comp2(df_history: pd.DataFrame, task_ids: list, epoch_limit = 50) -> None:\n    df = df_history[(df_history.task_id.isin(task_ids)) & (df_history.epoch <= epoch_limit)].copy()\n    df[\"loss_gap\"] = df.val_loss - df.loss\n    df_grp = df[[\"epoch\",\"task_id\", \"val_accuracy\", \"val_loss\", \"loss_gap\"]].groupby([\"epoch\", \"task_id\"]).mean()\n    fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(16, 8))\n    sns.lineplot(data=df_grp, x=\"epoch\", y=\"val_accuracy\", hue=\"task_id\",  ax=ax1)\n    sns.lineplot(data=df_grp, x=\"epoch\", y=\"val_loss\", hue=\"task_id\",  ax=ax2)\n    sns.lineplot(data=df_grp, x=\"epoch\", y=\"loss_gap\", hue=\"task_id\",  ax=ax3)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T10:52:32.157138Z","iopub.execute_input":"2024-04-05T10:52:32.158127Z","iopub.status.idle":"2024-04-05T10:52:32.420150Z","shell.execute_reply.started":"2024-04-05T10:52:32.158093Z","shell.execute_reply":"2024-04-05T10:52:32.418737Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### `/main.py`","metadata":{}},{"cell_type":"code","source":"import datetime\nimport os\nimport pandas as pd\nimport tensorflow as tf\n\n# handle different structure Kaggle (Notebook) vs. Colab (Modules)\n# this wouldn't be kept in any \"production\" version.\ntry:\n    from AMLS_II_assignment23_24.data_processing import pre_processing as data\n    from AMLS_II_assignment23_24.model import util as model_util\n    from AMLS_II_assignment23_24.model.util import Params, ResultCollector\nexcept ModuleNotFoundError:\n    pass\n\nfrom docopt import docopt\nfrom pathlib import Path\n\ntf.random.set_seed(67890)\n\nDEFAULT_PARAMS = Params(255, 196, 50, 0.005, True, 5, False)\n## DEFAULT_PARAMS = model_util.Params(50, 196, 1, True, 5, False)\n#DEFAULT_PARAMS = Params(50, 196, 1, 0.005, True, 5, False)\nprint(DEFAULT_PARAMS)\n\nARTEFACTS_PATH = Path(\"artefacts\")\nARTEFACTS_PATH.mkdir(parents=True, exist_ok=True)\n\ncollector = ResultCollector(ARTEFACTS_PATH)\n\n# Process Data\nprint(\"==== Loading Data ====\")\ncwd = os.getcwd()\n# ds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), DEFAULT_PARAMS)\n# print(f\"Class Weights: {class_weights}\")\n\n# print(\"==== Task A: Baseline Model ====\")\n\n# model = create_convnext_base(DEFAULT_PARAMS)\n# df_train, df_test = run_task(\"A_base\", model, ds_train, ds_valid, ds_test, DEFAULT_PARAMS)\n# collector.add_task_results(df_train, df_test)\n\n# print(\"==== Task B: Baseline + Data Augmentation ====\")\n# {\n#     \"\"\"\n#     Per task A, but with data augmentation.\n#     \"\"\"\n#     ds_train_aug = augment_dataset(ds_train, 2)\n#     model = create_convnext_base(DEFAULT_PARAMS)\n#     df_train, df_test = run_task(\"B_base_aug\", model, ds_train_aug, ds_valid, ds_test, DEFAULT_PARAMS)\n#     collector.add_task_results(df_train, df_test)    \n# }\n\n# print(\"==== Task C: Baseline + Data Augmentation + Class Weights ====\")\n# model = create_convnext_base(DEFAULT_PARAMS)\n# {\n#     \"\"\"\n#     Per task B but, given the large class imbalance, class weight supplied.\n#     \"\"\"\n#     ds_train_aug = augment_dataset(ds_train, 2)\n#     df_train, df_test = run_task(\"C_base_aug_wgts\", model, ds_train_aug, ds_valid, ds_test, DEFAULT_PARAMS, class_weights)\n#     collector.add_task_results(df_train, df_test)    \n# }\n\n# print(\"==== Task D: Baseline + Data Augmentation + Class Weights + Fine Tune ====\")\n# {\n#     \"\"\"\n#     Per task C but, given the large class imbalance, class weight supplied.\n#     \"\"\"\n#     fine_tune_params = Params(50, 196, 1, 1e-5, True, 5, False)\n#     print(fine_tune_params)\n#     model.base_model.trainable = True\n#     ds_train_aug = augment_dataset(ds_train, 2)\n#     df_train, df_test = run_task(\"D_base_aug_wgts_ft\", model, ds_train_aug, ds_valid, ds_test, DEFAULT_PARAMS, class_weights)\n#     collector.add_task_results(df_train, df_test)    \n# }\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T10:52:32.422223Z","iopub.execute_input":"2024-04-05T10:52:32.422623Z","iopub.status.idle":"2024-04-05T10:52:32.447234Z","shell.execute_reply.started":"2024-04-05T10:52:32.422592Z","shell.execute_reply":"2024-04-05T10:52:32.445528Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Params(image_size=255, batch_size=196, epochs=50, epsilon=0.005, early_stopping=True, early_stopping_patience=5, adjust_learning_rate=False)\n==== Loading Data ====\n","output_type":"stream"}]},{"cell_type":"code","source":"i = 1","metadata":{"execution":{"iopub.status.busy":"2024-04-05T10:52:32.450967Z","iopub.execute_input":"2024-04-05T10:52:32.451546Z","iopub.status.idle":"2024-04-05T10:52:32.458901Z","shell.execute_reply.started":"2024-04-05T10:52:32.451510Z","shell.execute_reply":"2024-04-05T10:52:32.457503Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"assert 1==2, \"stop here\"","metadata":{"execution":{"iopub.status.busy":"2024-04-05T10:52:32.460733Z","iopub.execute_input":"2024-04-05T10:52:32.461117Z","iopub.status.idle":"2024-04-05T10:52:32.550527Z","shell.execute_reply.started":"2024-04-05T10:52:32.461088Z","shell.execute_reply":"2024-04-05T10:52:32.549032Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop here\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","\u001b[0;31mAssertionError\u001b[0m: stop here"],"ename":"AssertionError","evalue":"stop here","output_type":"error"}]},{"cell_type":"code","source":"i = 2","metadata":{"execution":{"iopub.status.busy":"2024-04-05T10:52:32.551855Z","iopub.status.idle":"2024-04-05T10:52:32.552331Z","shell.execute_reply.started":"2024-04-05T10:52:32.552112Z","shell.execute_reply":"2024-04-05T10:52:32.552132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for bs in [64, 128, 192, 256]:\n    print(f\"Batch Size: {bs}\")\n    print(f\"Start: {datetime.datetime.now()}\")\n    batch_size_params = Params(255, bs, 50, 0.005, True, 5, False)\n    print(batch_size_params)\n    ds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), batch_size_params)\n    model = create_model(tf.keras.applications.ConvNeXtTiny, \"base\", batch_size_params)\n    df_train, df_test = run_task(f\"A_bs_{bs}\", model, ds_train, ds_valid, ds_test, batch_size_params)\n    collector.add_task_results(df_train, df_test)\n    print(model.model.evaluate(ds_test))\n    print(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for bs in [512, 768, 1024]:\n    print(f\"Batch Size: {bs}\")\n    print(f\"Start: {datetime.datetime.now()}\")\n    batch_size_params = Params(255, bs, 50, 0.005, True, 5, False)\n    print(batch_size_params)\n    ds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), batch_size_params)\n    model = create_model(tf.keras.applications.ConvNeXtTiny, \"base\", batch_size_params)\n    df_train, df_test = run_task(f\"A_bs_{bs}\", model, ds_train, ds_valid, ds_test, batch_size_params)\n    collector.add_task_results(df_train, df_test)\n    print(model.model.evaluate(ds_test))\n    print(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collector.get_test_scores()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf_res = pd.read_csv(\"/kaggle/working/artefacts/train_details.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_experiments_comp2(collector.get_train_details(), collector.get_train_details().task_id.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_experiments_comp2(df_res, df_res.task_id.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for eps in [0.0025, 0.0050, 0.0075, 0.01]:\n    print(f\"Epsilon: {eps}\")\n    print(f\"Start: {datetime.datetime.now()}\")\n    params = Params(255, 256, 50, eps, True, 5, False)\n    print(params)\n    ds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\n    model = create_model(tf.keras.applications.ConvNeXtTiny, \"base\", params)\n    df_train, df_test = run_task(f\"B_eps_{eps}\", model, ds_train, ds_valid, ds_test, params)\n    collector.add_task_results(df_train, df_test)\n    print(model.model.evaluate(ds_test))\n    print(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collector.get_test_scores()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for eps in [0.006, 0.007, 0.008]:\n    print(f\"Epsilon: {eps}\")\n    print(f\"Start: {datetime.datetime.now()}\")\n    params = Params(255, 256, 50, eps, True, 5, False)\n    print(params)\n    ds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\n    model = create_model(tf.keras.applications.ConvNeXtTiny, \"base\", params)\n    df_train, df_test = run_task(f\"B_eps_{eps}\", model, ds_train, ds_valid, ds_test, params)\n    collector.add_task_results(df_train, df_test)\n    print(model.model.evaluate(ds_test))\n    print(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collector.get_test_scores()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_experiments_comp2(collector.get_train_details(), collector.get_train_details().task_id.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for lr in [True, False]:\n    print(f\"LR: {lr}\")\n    print(f\"Start: {datetime.datetime.now()}\")\n    params = Params(255, 256, 50, 0.0075, True, 5, lr)\n    print(params)\n    ds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\n    model = create_model(tf.keras.applications.ConvNeXtTiny, \"base\", params)\n    df_train, df_test = run_task(f\"C_{lr}\", model, ds_train, ds_valid, ds_test, params)\n    collector.add_task_results(df_train, df_test)\n    print(model.model.evaluate(ds_test))\n    print(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\nfor m in [tf.keras.applications.ConvNeXtTiny, tf.keras.applications.ConvNeXtSmall, tf.keras.applications.ConvNeXtBase]:\n    print(f\"Model: {m}\")\n    print(f\"Start: {datetime.datetime.now()}\")\n    params = Params(255, 256, 50, 0.0075, True, 5, False)\n    print(params)\n    ds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\n    model = create_model(m, \"base\", params)\n    df_train, df_test = run_task(f\"C_{str(i)}\", model, ds_train, ds_valid, ds_test, params)\n    collector.add_task_results(df_train, df_test)\n    print(model.model.evaluate(ds_test))\n    print(f\"End: {datetime.datetime.now()}\")\n    i = i + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_experiments_comp2(collector.get_train_details(), [\"C_0\", \"C_1\", \"C_2\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collector.get_test_scores()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Start: {datetime.datetime.now()}\")\nparams = Params(255, 256, 50, 0.0075, True, 7, False)\nprint(params)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\nmodel = create_model(tf.keras.applications.ConvNeXtBase, \"base\", params)\ndf_train, df_test = run_task(\"D_wgts\", model, ds_train, ds_valid, ds_test, params, class_weights)\ncollector.add_task_results(df_train, df_test)\nprint(model.model.evaluate(ds_test))\nprint(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collector.get_test_scores()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\nfor m in [tf.keras.applications.ConvNeXtTiny, tf.keras.applications.EfficientNetB0]:\n    print(f\"Model: {m}\")\n    print(f\"Start: {datetime.datetime.now()}\")\n    params = Params(255, 256, 50, 0.0075, True, 5, False)\n    print(params)\n    ds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\n    model = create_model(m, \"base\", params)\n    df_train, df_test = run_task(f\"E_{str(i)}\", model, ds_train, ds_valid, ds_test, params, class_weights)\n    collector.add_task_results(df_train, df_test)\n    print(model.model.evaluate(ds_test))\n    print(f\"End: {datetime.datetime.now()}\")\n    i = i + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 2\nfor m in [tf.keras.applications.EfficientNetV2B0]:\n    print(f\"Model: {m}\")\n    print(f\"Start: {datetime.datetime.now()}\")\n    params = Params(255, 256, 50, 0.0075, True, 5, False)\n    print(params)\n    ds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\n    model = create_model(m, \"base\", params)\n    df_train, df_test = run_task(f\"E_{str(i)}\", model, ds_train, ds_valid, ds_test, params, class_weights)\n    collector.add_task_results(df_train, df_test)\n    print(model.model.evaluate(ds_test))\n    print(f\"End: {datetime.datetime.now()}\")\n    i = i + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_experiments_comp2(collector.get_train_details(), [\"E_0\", \"E_1\", \"E_2\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m1 = create_model_ablations(tf.keras.applications.ConvNeXtTiny, \"base\", params, 2, 1024)\nm1.model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\nparams = Params(255, 256, 50, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\nfor m in [tf.keras.applications.ConvNeXtTiny, tf.keras.applications.ConvNeXtBase,\n          tf.keras.applications.EfficientNetV2B0, tf.keras.applications.EfficientNetV2M]:\n    print(f\"Model: {m}\")\n    print(f\"Start: {datetime.datetime.now()}\")\n    print(params)\n    model = create_model(m, \"base\", params)\n    df_train, df_test = run_task(f\"C_{str(i)}\", model, ds_train, ds_valid, ds_test, params, class_weights)\n    collector.add_task_results(df_train, df_test)\n    print(model.model.evaluate(ds_test))\n    print(f\"End: {datetime.datetime.now()}\")\n    i = i + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collector.get_test_scores()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_experiments_comp2(collector.get_train_details(), [\"C_0\", \"C_1\", \"C_2\", \"C_3\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\nparams = Params(255, 256, 50, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\nds_train_aug = augment_dataset(ds_train, 2)\nfor m in [tf.keras.applications.ConvNeXtTiny, tf.keras.applications.EfficientNetV2B0]:\n    print(f\"Model: {m}\")\n    print(f\"Start: {datetime.datetime.now()}\")\n    print(params)\n    model = create_model(m, \"base\", params)\n    df_train, df_test = run_task(f\"D_{str(i)}\", model, ds_train_aug, ds_valid, ds_test, params, class_weights)\n    collector.add_task_results(df_train, df_test)\n    print(model.model.evaluate(ds_test))\n    print(f\"End: {datetime.datetime.now()}\")\n    i = i + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# out of memory ^\ni = 1\nparams = Params(255, 256, 50, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\nds_train_aug = augment_dataset(ds_train, 2)\nfor m in [tf.keras.applications.EfficientNetV2B0]:\n    print(f\"Model: {m}\")\n    print(f\"Start: {datetime.datetime.now()}\")\n    print(params)\n    model = create_model(m, \"base\", params)\n    df_train, df_test = run_task(f\"D_{str(i)}\", model, ds_train_aug, ds_valid, ds_test, params, class_weights)\n    collector.add_task_results(df_train, df_test)\n    print(model.model.evaluate(ds_test))\n    print(f\"End: {datetime.datetime.now()}\")\n    i = i + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = Params(255, 256, 50, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\nprint(f\"#FC: 1 - 1024\")\nprint(f\"Start: {datetime.datetime.now()}\")\nprint(params)\nmodel = create_model_ablations(tf.keras.applications.EfficientNetV2B0, \"base\", params, 1, 1024)\nprint(model.model.summary())\ndf_train, df_test = run_task(f\"E_1_1024\", model, ds_train, ds_valid, ds_test, params, class_weights)\ncollector.add_task_results(df_train, df_test)\nprint(model.model.evaluate(ds_test))\nprint(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = Params(255, 256, 50, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\nprint(f\"No Class Weights\")\nprint(f\"Start: {datetime.datetime.now()}\")\nprint(params)\nmodel = create_model_ablations(tf.keras.applications.EfficientNetV2B0, \"base\", params, 2, 1024)\nprint(model.model.summary())\ndf_train, df_test = run_task(f\"F_no_wgts\", model, ds_train, ds_valid, ds_test, params)\ncollector.add_task_results(df_train, df_test)\nprint(model.model.evaluate(ds_test))\nprint(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Task G - Custom Model","metadata":{}},{"cell_type":"code","source":"params = Params(255, 256, 50, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\nprint(f\"Custom Model\")\nprint(f\"Start: {datetime.datetime.now()}\")\nprint(params)\nmodel = create_simple_model(params)\nprint(model.model.summary())\ndf_train, df_test = run_task(f\"G_cust_model\", model, convert_dataset(ds_train), convert_dataset(ds_valid), convert_dataset(ds_test), params)\ncollector.add_task_results(df_train, df_test)\nprint(model.model.evaluate(ds_test))\nprint(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Task H: Fine-Tune","metadata":{}},{"cell_type":"code","source":"params = Params(255, 256, 50, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\nprint(f\"Fine Tune\")\nprint(f\"Start: {datetime.datetime.now()}\")\nprint(params)\nmodel = create_model_ablations(tf.keras.applications.EfficientNetV2B0, \"base\", params, 2, 1024)\nprint(model.model.summary())\ndf_train, df_test = run_task(f\"H_std\", model, ds_train, ds_valid, ds_test, params, class_weights)\ncollector.add_task_results(df_train, df_test)\nprint(model.model.evaluate(ds_test))\nprint(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.base_model.trainable = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = Params(255, 256, 50, 1e-5, True, 7, False)\nprint(f\"Fine Tune\")\nprint(f\"Start: {datetime.datetime.now()}\")\nprint(params)\ndf_train, df_test = run_task(f\"H_tune\", model, ds_train, ds_valid, ds_test, params, class_weights)\ncollector.add_task_results(df_train, df_test)\nprint(model.model.evaluate(ds_test))\nprint(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = Params(255, 256, 50, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\nprint(f\"Crop\")\nprint(f\"Start: {datetime.datetime.now()}\")\nprint(params)\nmodel = create_model_ablations(tf.keras.applications.EfficientNetV2B0, \"base\", params, 2, 1024)\nprint(model.model.summary())\ndf_train, df_test = run_task(f\"I\", model, ds_train, ds_valid, ds_test, params, class_weights)\ncollector.add_task_results(df_train, df_test)\nprint(model.model.evaluate(ds_test))\nprint(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = Params(255, 256, 50, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\nprint(f\"Batch Norm\")\nprint(f\"Start: {datetime.datetime.now()}\")\nprint(params)\nmodel = create_model_ablations(tf.keras.applications.EfficientNetV2B0, \"base\", params, 2, 1024)\nprint(model.model.summary())\ndf_train, df_test = run_task(f\"J\", model, ds_train, ds_valid, ds_test, params, class_weights)\ncollector.add_task_results(df_train, df_test)\nprint(model.model.evaluate(ds_test))\nprint(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = image_dataset_from_directory(\n        \"/kaggle/working/data/train/0/\",\n        labels=list(np.repeat(0, 663)),\n        label_mode='categorical',\n        image_size=[255, 255],\n        batch_size=256,\n        seed=12345,\n        shuffle=True,\n        crop_to_aspect_ratio=True\n    )\nds_aug = augment_dataset(ds, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\nds_train_over = tf.data.Dataset.concatenate(ds_train, ds)\nds_train_over = ds_train_over.shuffle(20000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def augment_dataset2(ds: Dataset, num_repeats: int) -> Dataset:\n    \"\"\"\n    \"\"\"\n    def augment(image, label):\n        seed = 12345\n        image = tf.image.random_flip_left_right(image, seed)\n        image = tf.image.random_flip_up_down(image, seed)\n        image = tf.image.random_brightness(image, 0.2, seed)\n        return image, label\n\n    return (\n        ds\n        .repeat(num_repeats)\n        .map(augment)\n    )\n\nparams = Params(255, 256, 50, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\n\nds_over = ds_train.unbatch().filter(lambda x, label: tf.equal(tf.argmax(label, axis=0), 0))\nds_aug = augment_dataset2(ds_over, 2)\n\nds_train_over = tf.data.Dataset.concatenate(ds_train.unbatch(), ds_aug)\nds_train_over = ds_train_over.shuffle(100000)\nds_train_over = ds_train_over.batch(256)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def augment_dataset2(ds: Dataset, num_repeats: int) -> Dataset:\n    \"\"\"\n    \"\"\"\n    def augment(image, label):\n        seed = 12345\n        image = tf.image.random_flip_left_right(image, seed)\n        image = tf.image.random_flip_up_down(image, seed)\n        image = tf.image.random_brightness(image, 0.2, seed)\n        return image, label\n\n    return (\n        ds\n        .repeat(num_repeats)\n        .map(augment)\n    )\n\nparams = Params(255, 256, 50, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\n\nds_over = ds_train.unbatch().filter(lambda x, label: tf.equal(tf.argmax(label, axis=0), 0))\nds_aug = augment_dataset2(ds_over, 2)\n\nds_train_over = tf.data.Dataset.concatenate(ds_train.unbatch(), ds_aug)\nds_train_over = ds_train_over.shuffle(100000)\nds_train_over = ds_train_over.batch(256)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Over-sample\")\nprint(f\"Start: {datetime.datetime.now()}\")\nprint(params)\nmodel = create_model_ablations(tf.keras.applications.EfficientNetV2B0, \"base\", params, 2, 1024)\nprint(model.model.summary())\ndf_train, df_test = run_task(f\"K_1\", model, ds_train_over, ds_valid, ds_test, params, class_weights)\ncollector.add_task_results(df_train, df_test)\nprint(model.model.evaluate(ds_test))\nprint(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def augment_dataset2(ds: Dataset, num_repeats: int) -> Dataset:\n    \"\"\"\n    \"\"\"\n    def augment(image, label):\n        seed = 12345\n        image = tf.image.random_flip_left_right(image, seed)\n        image = tf.image.random_flip_up_down(image, seed)\n        image = tf.image.random_brightness(image, 0.2, seed)\n        return image, label\n\n    return (\n        ds\n        .repeat(num_repeats)\n        .map(augment)\n    )\n\nparams = Params(255, 256, 50, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\n\nds_over = ds_train.unbatch().filter(lambda x, label: tf.equal(tf.argmax(label, axis=0), 0))\n#ds_aug = augment_dataset2(ds_over, 2)\n\nds_train_over = tf.data.Dataset.concatenate(ds_train.unbatch(), ds_over)\nds_train_over = ds_train_over.shuffle(100000)\nds_train_over = ds_train_over.batch(256)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Over-sample\")\nprint(f\"Start: {datetime.datetime.now()}\")\nprint(params)\nmodel = create_model_ablations(tf.keras.applications.EfficientNetV2B0, \"base\", params, 2, 1024)\nprint(model.model.summary())\ndf_train, df_test = run_task(f\"K_2\", model, ds_train_over, ds_valid, ds_test, params, class_weights)\ncollector.add_task_results(df_train, df_test)\nprint(model.model.evaluate(ds_test))\nprint(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = Params(255, 256, 50, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\nprint(f\"Over-sample\")\nprint(f\"Start: {datetime.datetime.now()}\")\nprint(params)\nmodel = create_model_ablations(tf.keras.applications.EfficientNetV2B0, \"base\", params, 2, 1024)\nprint(model.model.summary())\ndf_train, df_test = run_task(f\"K_3\", model, ds_train, ds_valid, ds_test, params, class_weights)\ncollector.add_task_results(df_train, df_test)\nprint(model.model.evaluate(ds_test))\nprint(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_experiments_comp2(collector.get_train_details(), [\"K_1\", \"K_2\", \"K_3\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(task_id: str, model: Model,\n             ds_train_: Dataset, ds_valid_: Dataset,\n             params: Params, weights = None) -> pd.DataFrame:\n\n    print(\"using adamw\")\n    model.compile(\n        optimizer=tf.keras.optimizers.AdamW(epsilon=params.epsilon),\n        loss=\"categorical_crossentropy\",\n        metrics=['accuracy']\n    )\n\n    early_stopping = callbacks.EarlyStopping(\n        min_delta=0.0001,\n        patience=params.early_stopping_patience,\n        restore_best_weights=True,\n        verbose = 1\n    )\n    \n    reduce_lr = callbacks.ReduceLROnPlateau(\n        monitor = 'val_loss', factor = 0.3, \n        patience = 3, min_delta = 0.0005, \n        mode = 'min', verbose = 1)\n    \n    cbs = []\n    if params.early_stopping:\n        print(\"Using EarlyStopping\")\n        cbs += [early_stopping]\n    if params.adjust_learning_rate:\n        print(\"Using ReduceLROnPlateau\")\n        cbs += [reduce_lr]\n\n    history = model.fit(\n        ds_train_,\n        validation_data=ds_valid_,\n        epochs=params.epochs,\n        verbose=1,\n        callbacks=cbs,\n        class_weight=weights\n    )\n   \n    df_hist = pd.DataFrame(history.history)\n    df_hist[\"task_id\"] = task_id\n    df_hist[\"epoch\"] = df_hist.index\n   \n    return df_hist","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = Params(255, 256, 50, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\nprint(f\"adamw\")\nprint(f\"Start: {datetime.datetime.now()}\")\nprint(params)\nmodel = create_model_ablations(tf.keras.applications.EfficientNetV2B0, \"base\", params, 2, 1024)\nprint(model.model.summary())\ndf_train, df_test = run_task(f\"L\", model, ds_train, ds_valid, ds_test, params, class_weights)\ncollector.add_task_results(df_train, df_test)\nprint(model.model.evaluate(ds_test))\nprint(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_experiments_comp2(collector.get_train_details(), [\"L\", \"K_3\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = Params(255, 256, 50, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\nprint(f\"reduced class 3 baseline\")\nprint(f\"Start: {datetime.datetime.now()}\")\nprint(params)\nmodel = create_model_ablations(tf.keras.applications.EfficientNetV2B0, \"base\", params, 2, 1024)\nprint(model.model.summary())\ndf_train, df_test = run_task(f\"M\", model, ds_train, ds_valid, ds_test, params, class_weights)\ncollector.add_task_results(df_train, df_test)\nprint(model.model.evaluate(ds_test))\nprint(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = Params(255, 256, 75, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\n\nds_over = ds_train.unbatch().filter(lambda x, label: tf.equal(tf.argmax(label, axis=0), 0))\n#ds_aug = augment_dataset2(ds_over, 2)\n\nds_train_over = tf.data.Dataset.concatenate(ds_train.unbatch(), ds_over)\nds_train_over = ds_train_over.shuffle(100000)\nds_train_over = ds_train_over.batch(256)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Over-sample\")\nprint(f\"Start: {datetime.datetime.now()}\")\nprint(params)\nmodel = create_model_ablations(tf.keras.applications.EfficientNetV2B0, \"base\", params, 2, 1024)\nprint(model.model.summary())\ndf_train, df_test = run_task(f\"M_1\", model, ds_train_over, ds_valid, ds_test, params)\ncollector.add_task_results(df_train, df_test)\nprint(model.model.evaluate(ds_test))\nprint(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = Params(255, 256, 50, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\nfor n in [256, 512, 2048]:\n    print(f\"FC #n: {n}\")\n    print(f\"Start: {datetime.datetime.now()}\")\n    print(params)\n    model = create_model_ablations(tf.keras.applications.EfficientNetV2B0, \"base\", params, 2, n)\n    print(model.model.summary())\n    df_train, df_test = run_task(f\"E_2_{str(n)}\", model, ds_train, ds_valid, ds_test, params, class_weights)\n    collector.add_task_results(df_train, df_test)\n    print(model.model.evaluate(ds_test))\n    print(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_experiments_comp2(collector.get_train_details(), [\"E_2_256\", \"E_2_512\", \"E_2_2048\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collector.get_test_scores()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = Params(255, 128, 50, 0.0075, True, 7, False)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\nfor n in [256, 512, 1024, 2048]:\n    print(f\"FC #n: {n}\")\n    print(f\"Start: {datetime.datetime.now()}\")\n    print(params)\n    model = create_model_ablations(tf.keras.applications.EfficientNetV2B0, \"base\", params, 1, n)\n    print(model.model.summary())\n    df_train, df_test = run_task(f\"E_1_{str(n)}\", model, ds_train, ds_valid, ds_test, params, class_weights)\n    collector.add_task_results(df_train, df_test)\n    print(model.model.evaluate(ds_test))\n    print(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_experiments_comp2(collector.get_train_details(), [\"E_1_256\", \"E_1_512\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Start: {datetime.datetime.now()}\")\nparams = Params(255, 256, 50, 0.0075, True, 7, False)\nprint(params)\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\nmodel = create_model(tf.keras.applications.EfficientNetB0, \"base\", params)\nds_train_aug = augment_dataset(ds_train, 2)\ndf_train, df_test = run_task(\"F\", model, ds_train_aug, ds_valid, ds_test, params, class_weights)\ncollector.add_task_results(df_train, df_test)\nprint(model.model.evaluate(ds_test))\nprint(f\"End: {datetime.datetime.now()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collector.get_test_scores()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.applications.EfficientNetB0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\nfor aug in [1, 2]:\n    print(f\"Repeats: {aug}\")\n    print(f\"Start: {datetime.datetime.now()}\")\n    params = Params(255, 256, 50, 0.0075, True, 5, False)\n    print(params)\n    ds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), params)\n    model = create_model(tf.keras.applications.ConvNeXtBase, \"base\", params)\n    df_train, df_test = run_task(f\"C_{str(i)}\", model, ds_train, ds_valid, ds_test, params)\n    collector.add_task_results(df_train, df_test)\n    print(model.model.evaluate(ds_test))\n    print(f\"End: {datetime.datetime.now()}\")\n    i = i + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(datetime.datetime.now())\nmodel_z = create_convnext_base(DEFAULT_PARAMS)\ndf_train, df_test = run_task(\"Z_base\", model_z, ds_train, ds_valid, ds_test, DEFAULT_PARAMS)\ncollector.add_task_results(df_train, df_test)\nprint(datetime.datetime.now())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(datetime.datetime.now())\nds_train_aug = augment_dataset(ds_train, 3)\nmodel_y = create_convnext_base(DEFAULT_PARAMS)\ndf_train, df_test = run_task(\"Y_base\", model_y, ds_train_aug, ds_valid, ds_test, DEFAULT_PARAMS)\ncollector.add_task_results(df_train, df_test)\nprint(datetime.datetime.now())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(datetime.datetime.now())\nds_train_aug = augment_dataset(ds_train, 2)\nmodel_b = create_convnext_base(DEFAULT_PARAMS)\ndf_train, df_test = run_task(\"B_base_aug\", model_b, ds_train_aug, ds_valid, ds_test, DEFAULT_PARAMS)\ncollector.add_task_results(df_train, df_test)\nprint(datetime.datetime.now())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_b.model.save(\"/kaggle/working/artefacts/model_b.keras\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model_b\ndel model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(datetime.datetime.now())\nds_train_aug = augment_dataset(ds_train, 2)\nmodel_c = create_convnext_base(DEFAULT_PARAMS)\ndf_train, df_test = run_task(\"C_base_aug_wgts\", model_c, ds_train_aug, ds_valid, ds_test, DEFAULT_PARAMS, class_weights)\ncollector.add_task_results(df_train, df_test)  \nprint(datetime.datetime.now())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_c.model.save(\"/kaggle/working/artefacts/model_c.keras\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_c.model.evaluate(ds_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(\"/kaggle/working/artefacts/test_scores.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collector.get_test_scores()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model_c","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(datetime.datetime.now())\nfine_tune_params = Params(50, 196, 1, 1e-5, True, 5, False)\nprint(fine_tune_params)\nmodel_c.base_model.trainable = True\ndf_train, df_test = run_task(\"D_base_aug_wgts_ft\", model_c, ds_train, ds_valid, ds_test, DEFAULT_PARAMS, class_weights)\ncollector.add_task_results(df_train, df_test)\nprint(datetime.datetime.now())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(datetime.datetime.now())\nds_train_aug = augment_dataset(ds_train, 2)\nmodel_d = create_model(tf.keras.applications.ConvNeXtTiny, \"baseline\", DEFAULT_PARAMS)\ndf_train, df_test = run_task(\"B_base_aug\", model_d, ds_train_aug, ds_valid, ds_test, DEFAULT_PARAMS)\ncollector.add_task_results(df_train, df_test)\nprint(datetime.datetime.now())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ARTEFACTS_PATH = Path(\"artefacts\")\nARTEFACTS_PATH.mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ARTEFACTS_PATH","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collector = ResultCollector(ARTEFACTS_PATH)\ncollector.add_task_results(df_train, df_test)\ncollector.add_task_results(df_train2, df_test2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collector.add_train_details(df_train)\ncollector.add_test_scores(df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collector.get_train_details()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collector.get_test_scores()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(ARTEFACTS_PATH / \"train_details.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(ARTEFACTS_PATH / \"test_scores.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`older_code`","metadata":{}},{"cell_type":"code","source":"#!rm -rf /kaggle/working/data\n#!rm /kaggle/working/data.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import keras\n\n# import pandas as pd\n# import random\n# import shutil\n# import tensorflow as tf\n# import os\n# import zipfile\n\n# from collections import Counter\n\n# from pathlib import Path\n# from sklearn.model_selection import train_test_split\n\n# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n# from tensorflow.keras import layers, callbacks\n# from tensorflow.keras.models import Model\n\n\n# from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D\n\n# from tensorflow.data import Dataset\n\n# from typing import NamedTuple, Tuple\n\n# import matplotlib.pyplot as plt\n# import seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def data_cleanup() -> None:\n#     # clean-up\n#     try:\n#         os.rmdir((data_path / \"train_images\"))\n#         os.remove(file)\n#     except Exception:\n#          pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = create_test_record(\"test_task\", [1.0, 2.0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.concat([df, create_test_record(\"test_task\", [1.0, 2.0])])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n\n# Large VGG-like model\ndef fatVGG(cifarInput, num_classes, name=\"vgg\"):\n    name = [name+str(i) for i in range(17)]\n    \n    # convolution and max pooling layers\n    vgg = Conv2D(32, (3, 3), padding='same', activation='relu', name=name[0])(cifarInput)\n    vgg = Conv2D(32, (3, 3), padding='same', activation='relu', name=name[1])(vgg)\n    vgg = MaxPooling2D(pool_size=(2,2), name=name[2])(vgg)\n    vgg = Dropout(0.25, name=name[3])(vgg)\n    vgg = Conv2D(64, (3, 3), padding='same', activation='relu', name=name[4])(vgg)\n    vgg = Conv2D(64, (3, 3), padding='same', activation='relu', name=name[5])(vgg)\n    vgg = MaxPooling2D(pool_size=(2,2), name=name[6])(vgg)\n    vgg = Dropout(0.25, name=name[7])(vgg)\n    vgg = Conv2D(128, (3, 3), padding='same', activation='relu', name=name[8])(vgg)\n    vgg = Conv2D(128, (3, 3), padding='same', activation='relu', name=name[9])(vgg)\n    vgg = Conv2D(128, (3, 3), padding='same', activation='relu', name=name[10])(vgg)\n    vgg = MaxPooling2D(pool_size=(2,2), name=name[11])(vgg)\n    vgg = Dropout(0.25, name=name[12])(vgg)\n\n    # classification layers\n    vgg = Flatten(name=name[13])(vgg)\n    vgg = Dense(512, activation='relu', name=name[14])(vgg)\n    vgg = Dropout(0.5, name=name[15])(vgg)\n    vgg = Dense(num_classes, activation='softmax', name=name[16])(vgg)\n    return vgg\n\n\ndef model_convnext_tiny(params: Params) -> Model:\n    \n    base_model = tf.keras.applications.ConvNeXtTiny(weights='imagenet', include_top=False)\n    base_model.trainable = False\n    \n    img_size = params.image_size\n    \n    return keras.Sequential([\n        \n        tf.keras.Input(shape=(img_size, img_size, 3)),\n        \n        base_model,\n        layers.GlobalAveragePooling2D(),\n\n        # Classifier Head\n        layers.Flatten(),\n        layers.Dense(1024, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(1024, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(units=5, activation=\"softmax\"),\n    ])\n\n\ndef model_convnext_base(params: Params) -> Model:\n    \n    base_model = tf.keras.applications.ConvNeXtBase(weights='imagenet', include_top=False)\n    base_model.trainable = False\n    \n    img_size = params.image_size\n    \n    return keras.Sequential([\n        \n        tf.keras.Input(shape=(img_size, img_size, 3)),\n        \n        base_model,\n        layers.GlobalAveragePooling2D(),\n\n        # Classifier Head\n        layers.Flatten(),\n        layers.Dense(1024, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(1024, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(units=5, activation=\"softmax\"),\n    ])\n\n\ndef model_convnext_tiny_1fc(params: Params) -> Model:\n    \n    base_model = tf.keras.applications.ConvNeXtTiny(weights='imagenet', include_top=False)\n    base_model.trainable = False\n    \n    img_size = params.image_size\n    \n    return keras.Sequential([\n        \n        tf.keras.Input(shape=(img_size, img_size, 3)),\n        \n        base_model,\n        layers.GlobalAveragePooling2D(),\n\n        # Classifier Head\n        layers.Flatten(),\n        layers.Dense(1024, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(units=5, activation=\"softmax\"),\n    ])\n\n\ndef model_effnet(params: Params) -> Model:\n    \n    base_model = tf.keras.applications.EfficientNetB0(weights='imagenet', include_top=False)\n    base_model.trainable = False\n    \n    img_size = params.image_size\n    \n    return keras.Sequential([\n        \n        tf.keras.Input(shape=(img_size, img_size, 3)),\n        \n        base_model,\n        layers.GlobalAveragePooling2D(),\n\n        # Classifier Head\n        layers.Flatten(),\n        layers.Dense(1024, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(1024, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(units=5, activation=\"softmax\"),\n    ])\n\n\ndef model_effnetv2(params: Params) -> Model:\n    \n    base_model = tf.keras.applications.EfficientNetV2B0(weights='imagenet', include_top=False)\n    base_model.trainable = False\n    \n    img_size = params.image_size\n    \n    return keras.Sequential([\n        \n        tf.keras.Input(shape=(img_size, img_size, 3)),\n        \n        base_model,\n        layers.GlobalAveragePooling2D(),\n\n        # Classifier Head\n        layers.Flatten(),\n        layers.Dense(1024, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(1024, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(units=5, activation=\"softmax\"),\n    ])\n\n\ndef effnet2(params: Params) -> Model:\n    base_model = tf.keras.applications.EfficientNetV2B0(weights='imagenet', include_top=False)\n    base_model.trainable = False\n    inputs = keras.Input(shape=(params.image_size, params.image_size, 3))\n    x = base_model(inputs, training=False)\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    x = keras.layers.Flatten()(x)\n    x = keras.layers.Dense(1024, activation=\"relu\")(x)\n    x = keras.layers.Dropout(0.5)(x)\n    x = keras.layers.Dense(1024, activation=\"relu\")(x)\n    x = keras.layers.Dropout(0.5)(x)\n    outputs = keras.layers.Dense(5, activation=\"softmax\")(x)\n    model = keras.Model(inputs, outputs)\n    return model\n    \n    \n#keras.utils.plot_model(mod.model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n\n\ndef train(task_id: str, model: Model,\n             ds_train_: Dataset, ds_valid_: Dataset,\n             params: Params, weights = None) -> pd.DataFrame:\n    \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(epsilon=0.005),\n        loss=\"categorical_crossentropy\",\n        metrics=['accuracy']\n    )\n\n    early_stopping = callbacks.EarlyStopping(\n        min_delta=0.0001,\n        patience=params.early_stopping_patience,\n        restore_best_weights=True,\n        verbose = 1\n    )\n    \n    reduce_lr = callbacks.ReduceLROnPlateau(\n        monitor = 'val_loss', factor = 0.3, \n        patience = 3, min_delta = 0.001, \n        mode = 'min', verbose = 1)\n    \n    cbs = []\n    if params.early_stopping:\n        print(\"Using EarlyStopping\")\n        cbs += [early_stopping]\n    if params.adjust_learning_rate:\n        print(\"Using ReduceLROnPlateau\")\n        cbs += [reduce_lr]\n\n    history = model.fit(\n        ds_train_,\n        validation_data=ds_valid_,\n        epochs=params.epochs,\n        verbose=1,\n        callbacks=cbs,\n        class_weight=weights\n    )\n   \n    df_hist = pd.DataFrame(history.history)\n    df_hist = df_hist.reset_index()\n    df_hist[\"task_id\"] = task_id\n    df_hist[\"epoch\"] = df_hist.index\n   \n    return df_hist\n\n\ndef run_experiment(exp_id: str, sub_exp_id: int, model_fn: str,\n                      ds_train_: Dataset, ds_valid_: Dataset, params: Params, weights = None) -> Tuple[Model, pd.DataFrame]:\n    \n    model = model_fn(params)\n    \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(epsilon=0.005),\n        loss=\"categorical_crossentropy\",\n        metrics=['accuracy']\n    )\n\n    early_stopping = callbacks.EarlyStopping(\n        min_delta=0.0001,\n        patience=params.early_stopping_patience,\n        restore_best_weights=True,\n        verbose = 1\n    )\n    \n    reduce_lr = callbacks.ReduceLROnPlateau(\n        monitor = 'val_loss', factor = 0.3, \n        patience = 3, min_delta = 0.001, \n        mode = 'min', verbose = 1)\n    \n    cbs = []\n    if params.early_stopping:\n        print(\"Using EarlyStopping\")\n        cbs += [early_stopping]\n    if params.adjust_learning_rate:\n        print(\"Using ReduceLROnPlateau\")\n        cbs += [reduce_lr]\n\n    history = model.fit(\n        ds_train_,\n        validation_data=ds_valid_,\n        epochs=params.epochs,\n        verbose=1,\n        callbacks=cbs,\n        class_weight=weights\n    )\n    \n    \n    \n    df_hist = pd.DataFrame(history.history)\n    df_hist = df_hist.reset_index()\n    df_hist[\"exp_id\"] = exp_id\n    df_hist[\"sub_exp_id\"] = sub_exp_id\n    df_hist[\"epoch\"] = df_hist.index\n   \n    return model, df_hist","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_results(df_all_results, df_new_results):\n    if df_all_results.empty:\n        return df_new_results\n    else:\n        return pd.concat([df_all_results, df_new_results])\n    \n\ndef add_test_results(df_all_results: pd.DataFrame, exp_id: str, res: list[float]):\n    df_res = pd.DataFrame({\"exp_id\": [exp_id], \"test_loss\" : [res[0]], \"test_accuracy\": [res[1]]})\n    if df_all_results.empty:\n        return df_res\n    else:\n        return pd.concat([df_all_results, df_res])\n\n\ndef recover_results() -> pd.DataFrame:\n    file = \"results.csv\"\n    if Path(file).exists():\n        return pd.read_csv(file)\n    return pd.DataFrame()\n\n\ndef recover_test_results() -> pd.DataFrame:\n    file = \"test_results.csv\"\n    if Path(file).exists():\n        return pd.read_csv(file)\n    return pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_experiment_results(df_history: pd.DataFrame, exp_id: str, epoch_limit = 50) -> None:\n    df = df_history[(df_history.exp_id == exp_id) & (df_history.epoch <= epoch_limit)].copy()\n    df[\"loss_gap\"] = df.val_loss - df.loss\n    df_loss = df[[\"epoch\", \"loss\", \"val_loss\"]].groupby([\"epoch\"]).mean()\n    df_loss_gap = df[[\"epoch\", \"loss_gap\"]].groupby([\"epoch\"]).mean()\n    df_acc = df[[\"epoch\", \"accuracy\", \"val_accuracy\"]].groupby([\"epoch\"]).mean()\n    df_loss.plot()\n    df_loss_gap.plot()\n    df_acc.plot();\n    \ndef plot_experiments_comp(df_history: pd.DataFrame, exp_id: list, y_dim: str, epoch_limit = 50) -> None:\n    df = df_history[(df_history.exp_id.isin(exp_id)) & (df_history.epoch <= epoch_limit)].copy()\n    df[\"loss_gap\"] = df.val_loss - df.loss\n    df_grp = df[[\"epoch\",\"exp_id\", y_dim]].groupby([\"epoch\", \"exp_id\"]).mean()\n    sns.lineplot(data=df_grp, x=\"epoch\", y=y_dim, hue=\"exp_id\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_experiments_comp2(df_history: pd.DataFrame, exp_id: list, epoch_limit = 50) -> None:\n    df = df_history[(df_history.exp_id.isin(exp_id)) & (df_history.epoch <= epoch_limit)].copy()\n    df[\"loss_gap\"] = df.val_loss - df.loss\n    df_grp = df[[\"epoch\",\"exp_id\", \"val_accuracy\", \"val_loss\", \"loss_gap\"]].groupby([\"epoch\", \"exp_id\"]).mean()\n    fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(16, 8))\n    sns.lineplot(data=df_grp, x=\"epoch\", y=\"val_accuracy\", hue=\"exp_id\",  ax=ax1)\n    sns.lineplot(data=df_grp, x=\"epoch\", y=\"val_loss\", hue=\"exp_id\",  ax=ax2)\n    sns.lineplot(data=df_grp, x=\"epoch\", y=\"loss_gap\", hue=\"exp_id\",  ax=ax3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"`main.py`","metadata":{}},{"cell_type":"code","source":"cwd = os.getcwd()\nds_train, ds_valid, ds_test, class_weights = data_preprocessing(Path(cwd), DEFAULT_PARAMS)\nprint(f\"Class Weights: {class_weights}\")\n\n#df_results = recover_results()\n#df_test_results = recover_test_results()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# augmented datasets\n#ds_train_aug = augment_dataset(ds_train, 1)\n#ds_train_aug_lg = augment_dataset(ds_train, 2)\n#ds_train_aug_xlg = augment_dataset(ds_train, 4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_results.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_results.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`experiments/tasks`","metadata":{}},{"cell_type":"code","source":"DEFAULT_PARAMS = Params(255, 196, 2, True, 5, False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ARTIFACTS_PATH = Path(\"artefacts\").mkdir(parents=True, exist_ok=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_task(task_id: str, model_wrapper: ModelWrapper,\n             ds_train_: Dataset, ds_valid_: Dataset, ds_test_: Dataset,\n             params: Params, weights = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \n    model = model_wrapper.model\n    # train\n    df_train = train(task_id, model, ds_train, ds_valid, DEFAULT_PARAMS)\n    # test\n    test_result = model.evaluate(ds_test)\n    df_test = create_test_record(task_id, test_result)\n    # save CM too\n    save_confusion_matrix(ds_test, model, task_id)\n    return df_train, df_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_test_record(task_id: str, result: list[float]):\n    return pd.DataFrame({\"task_id\": [task_id], \"test_loss\" : [result[0]], \"test_accuracy\": [result[1]]})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline_model = create_model(tf.keras.applications.ConvNeXtBase, \"baseline_model\", DEFAULT_PARAMS)\ndf_train, df_test = run_task(\"convnextbase\", baseline_model, ds_train, ds_valid, ds_test, DEFAULT_PARAMS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    df_train = run_task(\"model_convnext_tiny\", baseline_model.model,\n                   ds_train, ds_valid, DEFAULT_PARAMS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_results = add_results(df_results, df_hist)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline_model.model.evaluate(ds_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr = baseline_model.model.evaluate(ds_test)\ndf_test_results = add_test_results(df_test_results, \"convnext_tiny\", tr)\ntr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline_model.base_model.trainable = True\ndf_hist2 = run_task(\"convnext_tiny_ft\", baseline_model.model,\n                   ds_train, ds_valid, DEFAULT_PARAMS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr2 = baseline_model.model.evaluate(ds_test)\ndf_test_results = add_test_results(df_test_results, \"convnext_tiny_ft\", tr2)\ntr2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(ds_test, baseline_model.model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_hist = run_task(\"model_convnext_tiny\", baseline_model.model,\n                   ds_train, ds_valid, DEFAULT_PARAMS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(m, df_hist) = run_experiment(\"model_convnext_tiny\", 1, model_convnext_tiny, ds_train_aug_lg, ds_valid, DEFAULT_PARAMS)\ndf_results = add_results(df_results, df_hist)\n    \ndf_results.to_csv(\"results.csv\")\n\nm.evaluate(ds_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(m1, df_hist) = run_experiment(\"model_convnext_base_orig_img\", 1, model_convnext_base, augment_dataset(ds_train, 4), ds_valid, DEFAULT_PARAMS)\ndf_results = add_results(df_results, df_hist)\n    \ndf_results.to_csv(\"results.csv\")\n\nm1.evaluate(ds_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(m1a, df_hist) = run_experiment(\"model_convnext_base_orig_img_all\", 1, model_convnext_base, augment_dataset(ds_train, 1), ds_valid, DEFAULT_PARAMS, class_weights)\ndf_results = add_results(df_results, df_hist)\n    \ndf_results.to_csv(\"results.csv\")\n\nm1.evaluate(ds_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(m1b, df_hist) = run_experiment(\"model_convnext_base_orig_img_all_no_wgt\", 1, model_convnext_base, augment_dataset(ds_train, 1), ds_valid, DEFAULT_PARAMS)\ndf_results = add_results(df_results, df_hist)\n    \ndf_results.to_csv(\"results.csv\")\n\nm1b.evaluate(ds_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(m2, df_hist) = run_experiment(\"model_convnext_tiny_aug_sm\", 1, model_convnext_tiny, ds_train_aug, ds_valid, DEFAULT_PARAMS)\ndf_results = add_results(df_results, df_hist)\n\ndf_results.to_csv(\"results.csv\")\n\nm2.evaluate(ds_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_experiments_comp(df_results,[\"model_convnext_tiny\", \"model_convnext_tiny_aug\", \"model_convnext_tiny_aug4_sm\"], \"val_accuracy\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_experiments_comp(df_results,[\"model_convnext_tiny\", \"model_convnext_tiny_aug\"], \"val_loss\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(m3, df_hist) = run_experiment(\"model_convnext_tiny_aug4_sm\", 1, model_convnext_tiny, ds_train_aug_lg, ds_valid, DEFAULT_PARAMS)\ndf_results = add_results(df_results, df_hist)\n\ndf_results.to_csv(\"results.csv\")\n\nm3.evaluate(ds_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_results.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_experiments_comp(df_results,[\"model_convnext_tiny_aug\", \"model_convnext_tiny_sm\", \"model_convnext_tiny_aug_sm\", \"model_convnext_tiny_aug4_sm\"], \"val_accuracy\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(m4, df_hist) = run_experiment(\"model_convnext_tiny_1fc\", 1, model_convnext_tiny_1fc, ds_train_aug_lg, ds_valid, DEFAULT_PARAMS)\ndf_results = add_results(df_results, df_hist)\n\ndf_results.to_csv(\"results.csv\")\n\nm4.evaluate(ds_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_experiments_comp2(df_results,[\"model_convnext_tiny_1fc\", \"model_convnext_tiny_aug\", \"model_convnext_tiny_sm\", \"model_convnext_tiny_aug_sm\", \"model_convnext_tiny_aug4_sm\"], \"val_loss\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(m4, df_hist) = run_experiment(\"model_effnetv2\", 1, model_effnetv2, ds_train_aug_lg, ds_valid, DEFAULT_PARAMS)\ndf_results = add_results(df_results, df_hist)\n    \ndf_results.to_csv(\"results.csv\")\n\nm4.evaluate(ds_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(m5, df_hist) = run_experiment(\"model_effnet\", 1, model_effnet, ds_train_aug_lg, ds_valid, DEFAULT_PARAMS)\ndf_results = add_results(df_results, df_hist)\n\ndf_results.to_csv(\"results.csv\")\n\nr5 = m5.evaluate(ds_test)\ndf_test_results = add_test_results(df_test_results, \"model_effnet\", r5)\nprint(r5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(m6, df_hist) = run_experiment(\"model_effnet_wgts\", 1, model_effnet, ds_train_aug_lg, ds_valid, DEFAULT_PARAMS, class_weights)\ndf_results = add_results(df_results, df_hist)\n    \ndf_results.to_csv(\"results.csv\")\n\nr6 = m6.evaluate(ds_test)\ndf_test_results = add_test_results(df_test_results, \"model_effnet\", r6)\nprint(r6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(r6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(m7, df_hist) = run_experiment(\"effnet2\", 1, effnet2, ds_train_aug_lg, ds_valid, DEFAULT_PARAMS, class_weights)\ndf_results = add_results(df_results, df_hist)\n    \ndf_results.to_csv(\"results.csv\")\n\nr7 = m7.evaluate(ds_test)\ndf_test_results = add_test_results(df_test_results, \"effnet2\", r6)\nprint(r7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_experiments_comp2(df_results,[\"model_effnet\", \"model_effnet_wgts\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_experiments_comp2(df_results,[\"model_convnext_tiny_1fc\", \"model_convnext_tiny_aug\", \"model_effnetv2\", \"model_effnet\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = DEFAULT_PARAMS\nweights = class_weights\n    \nbase_model = tf.keras.applications.EfficientNetV2B0(weights='imagenet', include_top=False)\nbase_model.trainable = False\ninputs = keras.Input(shape=(params.image_size, params.image_size, 3))\nx = base_model(inputs, training=False)\nx = keras.layers.GlobalAveragePooling2D()(x)\nx = keras.layers.Flatten()(x)\n#x = keras.layers.Dense(1024, activation=\"relu\")(x)\n#x = keras.layers.Dropout(0.5)(x)\nx = keras.layers.Dense(1024, activation=\"relu\")(x)\nx = keras.layers.Dropout(0.5)(x)\noutputs = keras.layers.Dense(5, activation=\"softmax\")(x)\nmodel = keras.Model(inputs, outputs)\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(epsilon=0.005),\n    loss=\"categorical_crossentropy\",\n    metrics=['accuracy']\n)\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.0001,\n    patience=params.early_stopping_patience,\n    restore_best_weights=True,\n    verbose = 1\n)\n\nreduce_lr = callbacks.ReduceLROnPlateau(\n    monitor = 'val_loss', factor = 0.3, \n    patience = 3, min_delta = 0.001, \n    mode = 'min', verbose = 1)\n\ncbs = []\nif params.early_stopping:\n    print(\"Using EarlyStopping\")\n    cbs += [early_stopping]\nif params.adjust_learning_rate:\n    print(\"Using ReduceLROnPlateau\")\n    cbs += [reduce_lr]\n\nhistory = model.fit(\n    ds_train_aug_lg,\n    validation_data=ds_valid,\n    epochs=params.epochs,\n    verbose=1,\n    callbacks=cbs,\n    class_weight = None\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(ds_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model.trainable = True\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(epsilon=1e-5),\n    loss=\"categorical_crossentropy\",\n    metrics=['accuracy']\n)\n\nhistory2 = model.fit(\n    ds_train_aug_lg,\n    validation_data=ds_valid,\n    epochs=params.epochs,\n    verbose=1,\n    callbacks=cbs,\n    class_weight=None\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(ds_test)\n# [0.4968397915363312, 0.8268691301345825]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_eff = model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = DEFAULT_PARAMS\nweights = class_weights\n    \nbase_model = tf.keras.applications.ConvNeXtTiny(weights='imagenet', include_top=False)\nbase_model.trainable = False\ninputs = keras.Input(shape=(params.image_size, params.image_size, 3))\nx = base_model(inputs, training=False)\nx = keras.layers.GlobalAveragePooling2D()(x)\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(1024, activation=\"relu\")(x)\nx = keras.layers.Dropout(0.5)(x)\nx = keras.layers.Dense(1024, activation=\"relu\")(x)\nx = keras.layers.Dropout(0.5)(x)\noutputs = keras.layers.Dense(5, activation=\"softmax\")(x)\nmodel = keras.Model(inputs, outputs)\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(epsilon=0.005),\n    loss=\"categorical_crossentropy\",\n    metrics=['accuracy']\n)\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.0001,\n    patience=params.early_stopping_patience,\n    restore_best_weights=True,\n    verbose = 1\n)\n\nreduce_lr = callbacks.ReduceLROnPlateau(\n    monitor = 'val_loss', factor = 0.3, \n    patience = 3, min_delta = 0.001, \n    mode = 'min', verbose = 1)\n\ncbs = []\nif params.early_stopping:\n    print(\"Using EarlyStopping\")\n    cbs += [early_stopping]\nif params.adjust_learning_rate:\n    print(\"Using ReduceLROnPlateau\")\n    cbs += [reduce_lr]\n\nhistory = model.fit(\n    ds_train_aug_lg,\n    validation_data=ds_valid,\n    epochs=params.epochs,\n    verbose=1,\n    callbacks=cbs,\n    class_weight = weights\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(ds_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model.trainable = True\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(epsilon=1e-5),\n    loss=\"categorical_crossentropy\",\n    metrics=['accuracy']\n)\n\nhistory2 = model.fit(\n    ds_train_aug_lg,\n    validation_data=ds_valid,\n    epochs=params.epochs,\n    verbose=1,\n    callbacks=cbs,\n    class_weight = weights\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.model.evaluate(ds_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del ds_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(ds_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m1.evaluate(ds_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m1.evaluate(ds_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probabilities = model.model.predict(xs)\npredictions = np.argmax(probabilities, axis=1)\n#Counter(predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xs = np.concatenate([x for x, y in ds_test], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probabilities = m1.predict(ds_test2)\npredictions = np.argmax(probabilities, axis=1)\nCounter(predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probabilities = m1a.predict(ds_test2)\npredictions = np.argmax(probabilities, axis=1)\nCounter(predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = np.concatenate([y for x, y in ds_test], axis=0)\nys = [np.argmax(x) for x in y]\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n#Create confusion matrix and normalizes it over predicted (columns)\nresult = confusion_matrix(ys, predictions, labels=[0,1,2,3,4], normalize='pred')\nprint(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay(result, display_labels=[0,1,2,3,4])\ndisp.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_test2 = image_dataset_from_directory(\n        Path(\"/kaggle/working/data/test/\"),\n        labels='inferred',\n        label_mode='categorical',\n        image_size=[255, 255],\n        batch_size=196,\n        shuffle=False\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.model.evaluate(ds_test2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probabilities = model.model.predict(ds_test2)\npredictions = np.argmax(probabilities, axis=1)\n#Counter(predictions)\n\ny = np.concatenate([y for x, y in ds_test2], axis=0)\nys = [np.argmax(x) for x in y]\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n#Create confusion matrix and normalizes it over predicted (columns)\nresult = confusion_matrix(ys, predictions, labels=[0,1,2,3,4], normalize='pred')\ndisp = ConfusionMatrixDisplay(result, display_labels=[0,1,2,3,4])\ndisp.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef plot_confusion_matrix(ds: Dataset, model: Model) -> None:\n    probabilities = model.predict(ds)\n    predictions = np.argmax(probabilities, axis=1)\n\n    one_hot_labels = np.concatenate([y for x, y in ds], axis=0)\n    labels = [np.argmax(x) for x in one_hot_labels]\n    \n    result = confusion_matrix(labels, predictions, labels=[0,1,2,3,4], normalize='pred')\n    disp = ConfusionMatrixDisplay(result, display_labels=[0,1,2,3,4])\n    disp.plot()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m1b.evaluate(ds_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m1b.evaluate(ds_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(ds_test2, m1b)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}